{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the env_vars module\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from modules.env_vars import set_os_env_vars, check_missing_vars\n",
    "from modules.neon_db import run_neon_query, load_sql_query\n",
    "from modules.date_functions import get_current_date\n",
    "from modules.reference_extraction import create_content_from_df\n",
    "from modules.prompt_templates import one_shot_example, system_message_example\n",
    "\n",
    "set_os_env_vars() # This will execute the code in env_vars.py and put the environment variables in os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.langchain_config import set_langsmith_client, get_langsmith_tracer, get_llm_model, load_model_costs\n",
    "\n",
    "set_langsmith_client()\n",
    "tracer = get_langsmith_tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude-3-sonnet-20240229': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'claude-3-5-sonnet-20241022': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'gpt-4o-mini': {'provider': 'openai'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the costs\n",
    "MODEL_COSTS = load_model_costs()\n",
    "MODEL_COSTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "model_name = \"gpt-4o-mini\"\n",
    "streaming = True # Streaming is when the LLM returns a token at a time, instead of the entire response at once\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm_model(model_name, streaming, MODEL_COSTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>media_type</th>\n",
       "      <th>status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0762d1d-a825-4427-a785-cb52229f4c67</td>\n",
       "      <td>https://aidanmclaughlin.notion.site/reasoners-...</td>\n",
       "      <td>web-page</td>\n",
       "      <td>completed</td>\n",
       "      <td>2024-11-29 07:51:53.011015</td>\n",
       "      <td>Notion â€“ The all-in-one workspace for your not...</td>\n",
       "      <td>A new tool that blends your everyday work apps...</td>\n",
       "      <td>The article discusses the limitations of curre...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  b0762d1d-a825-4427-a785-cb52229f4c67   \n",
       "\n",
       "                                                 url media_type     status  \\\n",
       "0  https://aidanmclaughlin.notion.site/reasoners-...   web-page  completed   \n",
       "\n",
       "                  created_at  \\\n",
       "0 2024-11-29 07:51:53.011015   \n",
       "\n",
       "                                               title  \\\n",
       "0  Notion â€“ The all-in-one workspace for your not...   \n",
       "\n",
       "                                         description  \\\n",
       "0  A new tool that blends your everyday work apps...   \n",
       "\n",
       "                                             summary author published_at  \n",
       "0  The article discusses the limitations of curre...   None          NaT  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = load_sql_query(\"web_pages.sql\")\n",
    "df = run_neon_query(query)\n",
    "\n",
    "print(\"Number of rows:\", len(df.index))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "\n",
      "<START Article Number: 1>\n",
      "Title: Notion â€“ The all-in-one workspace for your notes, tasks, wikis, and databases.\n",
      "URL: https://aidanmclaughlin.notion.site/reasoners-problem\n",
      "Summary: The article discusses the limitations of current reasoning models, particularly OpenAI's o1, which utilize reinforcement learning (RL) to enhance reasoning capabilities. While these models show promise in structured environments with clear rewards, they struggle with open-ended tasks that lack frequent feedback, such as creative writing or philosophical reasoning. The author argues that despite the advancements in RL, these models do not generalize well beyond their training domains, leading to subpar performance in tasks requiring nuanced understanding. The piece highlights the challenges of scaling model size and the potential stagnation in AI development if the focus remains solely on improving reasoning without addressing the need for larger, more capable models. Key insights include the importance of transfer learning, the limitations of RL in sparse reward environments, and the need for models that can handle complex, unstructured tasks effectively.\n",
      "\n",
      "- Recognize that RL-based models excel in environments with clear rewards but falter in open-ended tasks.\n",
      "- Understand the limitations of transfer learning in current reasoning models, which do not generalize well across different domains.\n",
      "- Acknowledge the challenges in scaling model size and the potential for stagnation in AI advancements if focus remains narrow.\n",
      "- Consider the importance of developing models that can effectively tackle complex, unstructured problems beyond mathematical or coding tasks.\n",
      "Description: A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\n",
      "Created: 2024-11-29\n",
      "Type: web-page\n",
      "<END Article Number: 1>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the results (summary, titles, etc.)\n",
    "all_content, all_content_list = create_content_from_df(df)\n",
    "\n",
    "print(len(all_content_list))\n",
    "print(all_content_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_text(text):\n",
    "    return (\n",
    "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[0.009889289736747742, -0.005578675772994757, 0.00683477520942688, -0.03805781528353691, -0.01824733428657055, -0.04121600463986397, -0.007636285852640867, 0.03225184231996536, 0.018949154764413834, 9.352207416668534e-05]\n"
     ]
    }
   ],
   "source": [
    "test_embedding = emb_text(\"This is a test\")\n",
    "embedding_dim = len(test_embedding)\n",
    "print(embedding_dim)\n",
    "print(test_embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for the argument of MilvusClient:\n",
    "- Setting the uri as a local file, e.g../milvus.db, is the most convenient method, as it automatically utilizes Milvus Lite to store all data in this file.\n",
    "- If you have large scale of data, you can set up a more performant Milvus server on docker or kubernetes. In this setup, please use the server uri, e.g.http://localhost:19530, as your uri.\n",
    "- If you want to use Zilliz Cloud, the fully managed cloud service for Milvus, adjust the uri and token, which correspond to the Public Endpoint and Api key in Zilliz Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules.milvus_helper\n",
    "import importlib\n",
    "\n",
    "# import modules.milvus_wrapper\n",
    "# importlib.reload(modules.milvus_helper)\n",
    "\n",
    "# from modules.milvus_helper import (\n",
    "#     get_milvus_client, create_milvus_collection, create_demo_hybrid_milvus_schema, get_dense_embedding_details, create_demo_hybrid_milvus_indices\n",
    "# )\n",
    "\n",
    "import modules.milvus_wrapper\n",
    "importlib.reload(modules.milvus_wrapper)\n",
    "from modules.milvus_wrapper import MilvusLiteClient, MilvusFullClient, get_dense_embedding_details\n",
    "from pymilvus import utility\n",
    "\n",
    "milvus_lite_client = MilvusLiteClient()\n",
    "milvus_full_client = MilvusFullClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vector Database Implementation with Milvus Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_lite_client.create_collection(dimension=embedding_dim,\n",
    "                                     collection_name=\"my_rag_collection\",\n",
    "                                     metric_type=\"IP\", consistency_level=\"Strong\", drop_if_exists=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search Vector Database Implementation with Milvus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491fdfa2c1fa4aa3b6008f8f28ace6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense_dim, dense_embedding_function = get_dense_embedding_details(use_fp16=False, device=\"cpu\")\n",
    "schema = milvus_full_client.create_demo_hybrid_schema(embedding_dim=dense_dim)\n",
    "milvus_hybrid_collection = milvus_full_client.create_collection(collection_name=\"my_hybrid_collection\",\n",
    "                         schema=schema, consistency_level=\"Strong\", drop_if_exists=True)\n",
    "milvus_full_client.create_demo_hybrid_indices(milvus_hybrid_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_hybrid_collection: {'state': <LoadState: Loaded>}\n",
      "my_rag_collection: Loaded\n"
     ]
    }
   ],
   "source": [
    "# Check the load state of the collections\n",
    "res = milvus_lite_client.client.get_load_state(\n",
    "    collection_name=\"my_hybrid_collection\"\n",
    ")\n",
    "print(\"my_hybrid_collection:\", res)\n",
    "\n",
    "res = utility.load_state(\n",
    "    collection_name=\"my_rag_collection\"\n",
    ")\n",
    "print(\"my_rag_collection:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How I can speak English fluently?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"quora_duplicate_questions.tsv\"\n",
    "df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "questions = set()\n",
    "for _, row in df.iterrows():\n",
    "    obj = row.to_dict()\n",
    "    questions.add(obj[\"question1\"][:512])\n",
    "    questions.add(obj[\"question2\"][:512])\n",
    "    if len(questions) > 500:  # Skip this if you want to use the full dataset\n",
    "        break\n",
    "\n",
    "docs = list(questions)\n",
    "\n",
    "# example question\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde8b7dba51f40588d8208f2fb51f983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "M3Embedder.encode() missing 1 required positional argument: 'sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m ef \u001b[38;5;241m=\u001b[39m BGEM3EmbeddingFunction(use_fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dense_dim \u001b[38;5;241m=\u001b[39m ef\u001b[38;5;241m.\u001b[39mdim[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m docs_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/GenAI/newsletter/genai-newsletter/venv/lib/python3.11/site-packages/milvus_model/hybrid/bge_m3.py:76\u001b[0m, in \u001b[0;36mBGEM3EmbeddingFunction.__call__\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/GenAI/newsletter/genai-newsletter/venv/lib/python3.11/site-packages/milvus_model/hybrid/bge_m3.py:87\u001b[0m, in \u001b[0;36mBGEM3EmbeddingFunction._encode\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m---> 87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dense\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: M3Embedder.encode() missing 1 required positional argument: 'sentences'"
     ]
    }
   ],
   "source": [
    "from milvus_model.hybrid import BGEM3EmbeddingFunction\n",
    "\n",
    "ef = BGEM3EmbeddingFunction(use_fp16=False, device=\"cpu\")\n",
    "dense_dim = ef.dim[\"dense\"]\n",
    "\n",
    "docs_embeddings = ef(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "M3Embedder.encode() missing 1 required positional argument: 'sentences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Generate embeddings using BGE-M3 model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# docs_embeddings = dense_embedding_function(all_content_list)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m docs_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(docs_embeddings)\n",
      "File \u001b[0;32m~/Documents/Projects/GenAI/newsletter/genai-newsletter/venv/lib/python3.11/site-packages/milvus_model/hybrid/bge_m3.py:110\u001b[0m, in \u001b[0;36mBGEM3EmbeddingFunction.encode_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projects/GenAI/newsletter/genai-newsletter/venv/lib/python3.11/site-packages/milvus_model/hybrid/bge_m3.py:87\u001b[0m, in \u001b[0;36mBGEM3EmbeddingFunction._encode\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[0;32m---> 87\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dense\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: M3Embedder.encode() missing 1 required positional argument: 'sentences'"
     ]
    }
   ],
   "source": [
    "# Generate embeddings using BGE-M3 model\n",
    "# docs_embeddings = dense_embedding_function(all_content_list)\n",
    "docs_embeddings = ef.encode_documents(docs)\n",
    "print(docs_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(docs), 50):\n",
    "    batched_entities = [\n",
    "        docs[i : i + 50],\n",
    "        docs_embeddings[\"sparse\"][i : i + 50],\n",
    "        docs_embeddings[\"dense\"][i : i + 50],\n",
    "    ]\n",
    "    col.insert(batched_entities)\n",
    "print(\"Number of entities inserted:\", col.num_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:29<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(all_content_list, desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'insert_count': 33, 'ids': [454319645845618754, 454319645845618755, 454319645845618756, 454319645845618757, 454319645845618758, 454319645845618759, 454319645845618760, 454319645845618761, 454319645845618762, 454319645845618763, 454319645845618764, 454319645845618765, 454319645845618766, 454319645845618767, 454319645845618768, 454319645845618769, 454319645845618770, 454319645845618771, 454319645845618772, 454319645845618773, 454319645845618774, 454319645845618775, 454319645845618776, 454319645845618777, 454319645845618778, 454319645845618779, 454319645845618780, 454319645845618781, 454319645845618782, 454319645845618783, 454319645845618784, 454319645845618785, 454319645845618786], 'cost': 0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_client.insert(collection_name=\"my_hybrid_collection\", data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the text lines, create embeddings, and then insert the data into Milvus.\n",
    "- Here is a new field text, which is a non-defined field in the collection schema. It will be automatically added to the reserved JSON dynamic field, which can be treated as a normal field at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:12<00:00,  2.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 33, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32], 'cost': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(all_content_list, desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which vector database should I use?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the question in the collection and retrieve the semantic top-3 matches\n",
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[\n",
    "        emb_text(question)\n",
    "    ],  # Use the `emb_text` function to convert the question to an embedding vector\n",
    "    limit=3,  # Return top 3 results\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "    output_fields=[\"text\"],  # Return the text field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"\\n<START Article Number: 13>\\nTitle: Binary vector embeddings are so cool\\nURL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\nSummary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings\\u2014which prioritize important information at the beginning of the vector\\u2014further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \\n\\n- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\\n- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\\n- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\\n- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\\nDescription: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression \\ud83e\\udd2f.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 13>\\n\",\n",
      "        0.43560677766799927\n",
      "    ],\n",
      "    [\n",
      "        \"\\n<START Article Number: 12>\\nTitle: Comparing full text search algorithms: BM25, TF-IDF, and Postgres\\nURL: https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\\nSummary: The comparison of full text search algorithms BM25, TF-IDF, and PostgreSQL's full text search highlights significant differences in their approaches to document relevance scoring. BM25 improves upon TF-IDF by incorporating a saturation function for term frequency, document length normalization, and a smoothed Inverse Document Frequency (IDF), which collectively enhance its ability to rank documents more effectively. In contrast, TF-IDF lacks these features, relying on simpler heuristics. When compared to PostgreSQL's full text search, BM25 offers a more sophisticated ranking mechanism that considers term rarity and document length, while PostgreSQL primarily uses stopword dictionaries and basic term frequency without saturation. The introduction of the `pg_bm25` extension for ParadeDB demonstrates the growing need for advanced search capabilities within PostgreSQL environments. Overall, while BM25 provides superior search quality, PostgreSQL's simplicity may appeal to certain applications. \\n\\n### Key Points:\\n- **BM25 vs TF-IDF:** BM25 introduces saturation for term frequency, document length normalization, and smoothed IDF, enhancing relevance scoring.\\n- **PostgreSQL Limitations:** PostgreSQL's full text search lacks the sophistication of BM25, relying on stopword dictionaries and basic term frequency.\\n- **Performance Considerations:** ParadeDB's `pg_bm25` extension offers BM25-based ranking, significantly improving search speed compared to native PostgreSQL.\\n- **Application Context:** Choose BM25 for applications requiring high search quality; opt for PostgreSQL for simpler implementations.\\nDescription: I wrote another post about  and had initially included comparisons with two other algorithms. However, that post was already quite long so here are the brief...\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 12>\\n\",\n",
      "        0.3415015935897827\n",
      "    ],\n",
      "    [\n",
      "        \"\\n<START Article Number: 26>\\nTitle: GitHub - circlemind-ai/fast-graphrag: RAG that intelligently adapts to your use case, data, and queries\\nURL: https://github.com/circlemind-ai/fast-graphrag\\nSummary: The Fast GraphRAG framework is designed for efficient, interpretable retrieval workflows, leveraging a graph-based approach to enhance data interaction and exploration. It supports dynamic data management, allowing for real-time updates and incremental changes, which is crucial for applications requiring adaptability to evolving datasets. The framework is built to be low-cost and efficient, making it suitable for large-scale implementations without significant resource overhead. Key features include intelligent exploration using PageRank algorithms, asynchronous operations, and comprehensive type support, which collectively facilitate robust and predictable workflows. Fast GraphRAG is open-source under the MIT license, promoting community contributions and collaboration. \\n\\n### Key Points for Technical Professionals and Product Developers:\\n- **Interpretable Knowledge Management**: Utilize graph structures for human-navigable knowledge that can be queried and visualized.\\n- **Cost Efficiency**: Achieve significant cost savings compared to traditional methods, especially as data size increases.\\n- **Dynamic and Incremental Updates**: Implement real-time data updates to maintain relevance and accuracy in applications.\\n- **Intelligent Exploration**: Leverage PageRank for enhanced accuracy in data retrieval and exploration.\\n- **Asynchronous and Typed Workflows**: Ensure robust operations with full type support for predictable outcomes.\\n- **Community Engagement**: Contribute to the open-source project to enhance functionality and share knowledge.\\nDescription: RAG that intelligently adapts to your use case, data, and queries - circlemind-ai/fast-graphrag\\nCreated: 2024-11-19\\nType: web-page\\n<END Article Number: 26>\\n\",\n",
      "        0.31278204917907715\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Print the retrieved lines with distances\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a LLM to answer the question using the retrieved lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<START Article Number: 13>\n",
      "Title: Binary vector embeddings are so cool\n",
      "URL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\n",
      "Summary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddingsâ€”which prioritize important information at the beginning of the vectorâ€”further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
      "\n",
      "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
      "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
      "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
      "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
      "Description: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression ðŸ¤¯.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 13>\n",
      "\n",
      "\n",
      "<START Article Number: 12>\n",
      "Title: Comparing full text search algorithms: BM25, TF-IDF, and Postgres\n",
      "URL: https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\n",
      "Summary: The comparison of full text search algorithms BM25, TF-IDF, and PostgreSQL's full text search highlights significant differences in their approaches to document relevance scoring. BM25 improves upon TF-IDF by incorporating a saturation function for term frequency, document length normalization, and a smoothed Inverse Document Frequency (IDF), which collectively enhance its ability to rank documents more effectively. In contrast, TF-IDF lacks these features, relying on simpler heuristics. When compared to PostgreSQL's full text search, BM25 offers a more sophisticated ranking mechanism that considers term rarity and document length, while PostgreSQL primarily uses stopword dictionaries and basic term frequency without saturation. The introduction of the `pg_bm25` extension for ParadeDB demonstrates the growing need for advanced search capabilities within PostgreSQL environments. Overall, while BM25 provides superior search quality, PostgreSQL's simplicity may appeal to certain applications. \n",
      "\n",
      "### Key Points:\n",
      "- **BM25 vs TF-IDF:** BM25 introduces saturation for term frequency, document length normalization, and smoothed IDF, enhancing relevance scoring.\n",
      "- **PostgreSQL Limitations:** PostgreSQL's full text search lacks the sophistication of BM25, relying on stopword dictionaries and basic term frequency.\n",
      "- **Performance Considerations:** ParadeDB's `pg_bm25` extension offers BM25-based ranking, significantly improving search speed compared to native PostgreSQL.\n",
      "- **Application Context:** Choose BM25 for applications requiring high search quality; opt for PostgreSQL for simpler implementations.\n",
      "Description: I wrote another post about  and had initially included comparisons with two other algorithms. However, that post was already quite long so here are the brief...\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 12>\n",
      "\n",
      "\n",
      "<START Article Number: 26>\n",
      "Title: GitHub - circlemind-ai/fast-graphrag: RAG that intelligently adapts to your use case, data, and queries\n",
      "URL: https://github.com/circlemind-ai/fast-graphrag\n",
      "Summary: The Fast GraphRAG framework is designed for efficient, interpretable retrieval workflows, leveraging a graph-based approach to enhance data interaction and exploration. It supports dynamic data management, allowing for real-time updates and incremental changes, which is crucial for applications requiring adaptability to evolving datasets. The framework is built to be low-cost and efficient, making it suitable for large-scale implementations without significant resource overhead. Key features include intelligent exploration using PageRank algorithms, asynchronous operations, and comprehensive type support, which collectively facilitate robust and predictable workflows. Fast GraphRAG is open-source under the MIT license, promoting community contributions and collaboration. \n",
      "\n",
      "### Key Points for Technical Professionals and Product Developers:\n",
      "- **Interpretable Knowledge Management**: Utilize graph structures for human-navigable knowledge that can be queried and visualized.\n",
      "- **Cost Efficiency**: Achieve significant cost savings compared to traditional methods, especially as data size increases.\n",
      "- **Dynamic and Incremental Updates**: Implement real-time data updates to maintain relevance and accuracy in applications.\n",
      "- **Intelligent Exploration**: Leverage PageRank for enhanced accuracy in data retrieval and exploration.\n",
      "- **Asynchronous and Typed Workflows**: Ensure robust operations with full type support for predictable outcomes.\n",
      "- **Community Engagement**: Contribute to the open-source project to enhance functionality and share knowledge.\n",
      "Description: RAG that intelligently adapts to your use case, data, and queries - circlemind-ai/fast-graphrag\n",
      "Created: 2024-11-19\n",
      "Type: web-page\n",
      "<END Article Number: 26>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the retrieved lines with a newline character\n",
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, if you're considering a vector database focused on efficient similarity searches and high retrieval accuracy, you should look into platforms that support binary quantized vector embeddings. These embeddings can achieve over 95% retrieval accuracy while compressing data significantly and accelerating retrieval speed. Additionally, combining binary quantization with techniques like Matryoshka embeddings can further enhance performance for applications that require fast and accurate vector similarity comparisons.\n",
      "\n",
      "On the other hand, if you're interested in full-text search capabilities with sophisticated relevance scoring, consider exploring systems that implement algorithms like BM25, as it provides better search quality than traditional methods like TF-IDF or basic PostgreSQL full-text search.\n",
      "\n",
      "For real-time adaptability and dynamic data management, the Fast GraphRAG framework mentioned could be a compelling option, especially for implementations that are low-cost and efficient at scale.\n",
      "\n",
      "Ultimately, the choice depends on your specific application needsâ€”whether you prioritize similarity searches (binary embeddings), document relevance scoring (BM25), or dynamic data interaction (Fast GraphRAG).\n"
     ]
    }
   ],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which vector database should I use?\n",
      "Context derived from the vector database: \n",
      "<START Article Number: 13>\n",
      "Title: Binary vector embeddings are so cool\n",
      "URL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\n",
      "Summary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddingsâ€”which prioritize important information at the beginning of the vectorâ€”further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
      "\n",
      "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
      "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
      "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
      "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
      "Description: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression ðŸ¤¯.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 13>\n",
      "\n",
      "\n",
      "<START Article Number: 12>\n",
      "Title: Comparing full text search algorithms: BM25, TF-IDF, and Postgres\n",
      "URL: https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\n",
      "Summary: The comparison of full text search algorithms BM25, TF-IDF, and PostgreSQL's full text search highlights significant differences in their approaches to document relevance scoring. BM25 improves upon TF-IDF by incorporating a saturation function for term frequency, document length normalization, and a smoothed Inverse Document Frequency (IDF), which collectively enhance its ability to rank documents more effectively. In contrast, TF-IDF lacks these features, relying on simpler heuristics. When compared to PostgreSQL's full text search, BM25 offers a more sophisticated ranking mechanism that considers term rarity and document length, while PostgreSQL primarily uses stopword dictionaries and basic term frequency without saturation. The introduction of the `pg_bm25` extension for ParadeDB demonstrates the growing need for advanced search capabilities within PostgreSQL environments. Overall, while BM25 provides superior search quality, PostgreSQL's simplicity may appeal to certain applications. \n",
      "\n",
      "### Key Points:\n",
      "- **BM25 vs TF-IDF:** BM25 introduces saturation for term frequency, document length normalization, and smoothed IDF, enhancing relevance scoring.\n",
      "- **PostgreSQL Limitations:** PostgreSQL's full text search lacks the sophistication of BM25, relying on stopword dictionaries and basic term frequency.\n",
      "- **Performance Considerations:** ParadeDB's `pg_bm25` extension offers BM25-based ranking, significantly improving search speed compared to native PostgreSQL.\n",
      "- **Application Context:** Choose BM25 for applications requiring high search quality; opt for PostgreSQL for simpler implementations.\n",
      "Description: I wrote another post about  and had initially included comparisons with two other algorithms. However, that post was already quite long so here are the brief...\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 12>\n",
      "\n",
      "\n",
      "<START Article Number: 26>\n",
      "Title: GitHub - circlemind-ai/fast-graphrag: RAG that intelligently adapts to your use case, data, and queries\n",
      "URL: https://github.com/circlemind-ai/fast-graphrag\n",
      "Summary: The Fast GraphRAG framework is designed for efficient, interpretable retrieval workflows, leveraging a graph-based approach to enhance data interaction and exploration. It supports dynamic data management, allowing for real-time updates and incremental changes, which is crucial for applications requiring adaptability to evolving datasets. The framework is built to be low-cost and efficient, making it suitable for large-scale implementations without significant resource overhead. Key features include intelligent exploration using PageRank algorithms, asynchronous operations, and comprehensive type support, which collectively facilitate robust and predictable workflows. Fast GraphRAG is open-source under the MIT license, promoting community contributions and collaboration. \n",
      "\n",
      "### Key Points for Technical Professionals and Product Developers:\n",
      "- **Interpretable Knowledge Management**: Utilize graph structures for human-navigable knowledge that can be queried and visualized.\n",
      "- **Cost Efficiency**: Achieve significant cost savings compared to traditional methods, especially as data size increases.\n",
      "- **Dynamic and Incremental Updates**: Implement real-time data updates to maintain relevance and accuracy in applications.\n",
      "- **Intelligent Exploration**: Leverage PageRank for enhanced accuracy in data retrieval and exploration.\n",
      "- **Asynchronous and Typed Workflows**: Ensure robust operations with full type support for predictable outcomes.\n",
      "- **Community Engagement**: Contribute to the open-source project to enhance functionality and share knowledge.\n",
      "Description: RAG that intelligently adapts to your use case, data, and queries - circlemind-ai/fast-graphrag\n",
      "Created: 2024-11-19\n",
      "Type: web-page\n",
      "<END Article Number: 26>\n",
      "\n",
      "Answer: Based on the provided context, if you're considering a vector database focused on efficient similarity searches and high retrieval accuracy, you should look into platforms that support binary quantized vector embeddings. These embeddings can achieve over 95% retrieval accuracy while compressing data significantly and accelerating retrieval speed. Additionally, combining binary quantization with techniques like Matryoshka embeddings can further enhance performance for applications that require fast and accurate vector similarity comparisons.\n",
      "\n",
      "On the other hand, if you're interested in full-text search capabilities with sophisticated relevance scoring, consider exploring systems that implement algorithms like BM25, as it provides better search quality than traditional methods like TF-IDF or basic PostgreSQL full-text search.\n",
      "\n",
      "For real-time adaptability and dynamic data management, the Fast GraphRAG framework mentioned could be a compelling option, especially for implementations that are low-cost and efficient at scale.\n",
      "\n",
      "Ultimately, the choice depends on your specific application needsâ€”whether you prioritize similarity searches (binary embeddings), document relevance scoring (BM25), or dynamic data interaction (Fast GraphRAG).\n"
     ]
    }
   ],
   "source": [
    "print('Question:', question)\n",
    "print('Context derived from the vector database:', context)\n",
    "print('Answer:', response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
