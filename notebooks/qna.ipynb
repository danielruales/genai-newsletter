{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the env_vars module\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from modules.env_vars import set_os_env_vars, check_missing_vars\n",
    "from modules.neon_db import run_neon_query, load_sql_query\n",
    "from modules.date_functions import get_current_date\n",
    "from modules.reference_extraction import create_content_from_df\n",
    "from modules.prompt_templates import one_shot_example, system_message_example\n",
    "\n",
    "set_os_env_vars() # This will execute the code in env_vars.py and put the environment variables in os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.langchain_config import set_langsmith_client, get_langsmith_tracer, get_llm_model, load_model_costs\n",
    "\n",
    "set_langsmith_client()\n",
    "tracer = get_langsmith_tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude-3-sonnet-20240229': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'claude-3-5-sonnet-20241022': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'gpt-4o-mini': {'provider': 'openai'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the costs\n",
    "MODEL_COSTS = load_model_costs()\n",
    "MODEL_COSTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "model_name = \"gpt-4o-mini\"\n",
    "streaming = True # Streaming is when the LLM returns a token at a time, instead of the entire response at once\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm_model(model_name, streaming, MODEL_COSTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>media_type</th>\n",
       "      <th>status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0762d1d-a825-4427-a785-cb52229f4c67</td>\n",
       "      <td>https://aidanmclaughlin.notion.site/reasoners-...</td>\n",
       "      <td>web-page</td>\n",
       "      <td>completed</td>\n",
       "      <td>2024-11-29 07:51:53.011015</td>\n",
       "      <td>Notion – The all-in-one workspace for your not...</td>\n",
       "      <td>A new tool that blends your everyday work apps...</td>\n",
       "      <td>The article discusses the limitations of curre...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  b0762d1d-a825-4427-a785-cb52229f4c67   \n",
       "\n",
       "                                                 url media_type     status  \\\n",
       "0  https://aidanmclaughlin.notion.site/reasoners-...   web-page  completed   \n",
       "\n",
       "                  created_at  \\\n",
       "0 2024-11-29 07:51:53.011015   \n",
       "\n",
       "                                               title  \\\n",
       "0  Notion – The all-in-one workspace for your not...   \n",
       "\n",
       "                                         description  \\\n",
       "0  A new tool that blends your everyday work apps...   \n",
       "\n",
       "                                             summary author published_at  \n",
       "0  The article discusses the limitations of curre...   None          NaT  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = load_sql_query(\"web_pages.sql\")\n",
    "df = run_neon_query(query)\n",
    "\n",
    "print(\"Number of rows:\", len(df.index))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "\n",
      "<START Article Number: 1>\n",
      "Title: Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.\n",
      "URL: https://aidanmclaughlin.notion.site/reasoners-problem\n",
      "Summary: The article discusses the limitations of current reasoning models, particularly OpenAI's o1, which utilize reinforcement learning (RL) to enhance reasoning capabilities. While these models show promise in structured environments with clear rewards, they struggle with open-ended tasks that lack frequent feedback, such as creative writing or philosophical reasoning. The author argues that despite the advancements in RL, these models do not generalize well beyond their training domains, leading to subpar performance in tasks requiring nuanced understanding. The piece highlights the challenges of scaling model size and the potential stagnation in AI development if the focus remains solely on improving reasoning without addressing the need for larger, more capable models. Key insights include the importance of transfer learning, the limitations of RL in sparse reward environments, and the need for models that can handle complex, unstructured tasks effectively.\n",
      "\n",
      "- Recognize that RL-based models excel in environments with clear rewards but falter in open-ended tasks.\n",
      "- Understand the limitations of transfer learning in current reasoning models, which do not generalize well across different domains.\n",
      "- Acknowledge the challenges in scaling model size and the potential for stagnation in AI advancements if focus remains narrow.\n",
      "- Consider the importance of developing models that can effectively tackle complex, unstructured problems beyond mathematical or coding tasks.\n",
      "Description: A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\n",
      "Created: 2024-11-29\n",
      "Type: web-page\n",
      "<END Article Number: 1>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the results (summary, titles, etc.)\n",
    "all_content, all_content_list, all_content_dict = create_content_from_df(df)\n",
    "\n",
    "print(len(all_content_list))\n",
    "print(all_content_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_text(text):\n",
    "    return (\n",
    "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[0.009889289736747742, -0.005578675772994757, 0.00683477520942688, -0.03805781528353691, -0.01824733428657055, -0.04121600463986397, -0.007636285852640867, 0.03225184231996536, 0.018949154764413834, 9.352207416668534e-05]\n"
     ]
    }
   ],
   "source": [
    "test_embedding = emb_text(\"This is a test\")\n",
    "embedding_dim = len(test_embedding)\n",
    "print(embedding_dim)\n",
    "print(test_embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As for the argument of MilvusClient:\n",
    "- Setting the uri as a local file, e.g../milvus.db, is the most convenient method, as it automatically utilizes Milvus Lite to store all data in this file.\n",
    "- If you have large scale of data, you can set up a more performant Milvus server on docker or kubernetes. In this setup, please use the server uri, e.g.http://localhost:19530, as your uri.\n",
    "- If you want to use Zilliz Cloud, the fully managed cloud service for Milvus, adjust the uri and token, which correspond to the Public Endpoint and Api key in Zilliz Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules.milvus_helper\n",
    "import importlib\n",
    "\n",
    "# import modules.milvus_wrapper\n",
    "# importlib.reload(modules.milvus_helper)\n",
    "\n",
    "# from modules.milvus_helper import (\n",
    "#     get_milvus_client, create_milvus_collection, create_demo_hybrid_milvus_schema, get_dense_embedding_details, create_demo_hybrid_milvus_indices\n",
    "# )\n",
    "\n",
    "import modules.milvus_wrapper\n",
    "importlib.reload(modules.milvus_wrapper)\n",
    "from modules.milvus_wrapper import MilvusLiteClient, MilvusFullClient, get_dense_embedding_details\n",
    "from pymilvus import utility\n",
    "\n",
    "milvus_lite_client = MilvusLiteClient()\n",
    "milvus_full_client = MilvusFullClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vector Database Implementation with Milvus Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_lite_client.create_collection(dimension=embedding_dim,\n",
    "                                     collection_name=\"my_rag_collection\",\n",
    "                                     metric_type=\"IP\", consistency_level=\"Strong\", drop_if_exists=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search Vector Database Implementation with Milvus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f03844b0fe44355bed452d04ac51770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dense_dim, dense_embedding_function = get_dense_embedding_details(use_fp16=False, device=\"cpu\")\n",
    "schema = milvus_full_client.create_demo_hybrid_schema(embedding_dim=dense_dim)\n",
    "milvus_hybrid_collection = milvus_full_client.create_collection(collection_name=\"my_hybrid_collection\",\n",
    "                         schema=schema, consistency_level=\"Strong\", drop_if_exists=True)\n",
    "milvus_full_client.create_demo_hybrid_indices(milvus_hybrid_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_hybrid_collection: {'state': <LoadState: Loaded>}\n",
      "my_rag_collection: Loaded\n"
     ]
    }
   ],
   "source": [
    "# Check the load state of the collections\n",
    "res = milvus_lite_client.client.get_load_state(\n",
    "    collection_name=\"my_hybrid_collection\"\n",
    ")\n",
    "print(\"my_hybrid_collection:\", res)\n",
    "\n",
    "res = utility.load_state(\n",
    "    collection_name=\"my_rag_collection\"\n",
    ")\n",
    "print(\"my_rag_collection:\", res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dense': [array([-0.06493555,  0.00202186, -0.0297969 , ..., -0.01162049,\n",
      "        0.00279936, -0.03092   ], dtype=float32), array([-0.02462117,  0.00991646, -0.05236292, ..., -0.01837842,\n",
      "       -0.00888776,  0.01576928], dtype=float32), array([-0.02958542,  0.00172268, -0.02052973, ..., -0.04473894,\n",
      "        0.06390611, -0.00569023], dtype=float32), array([-0.02300773, -0.03120776, -0.01790264, ...,  0.00014177,\n",
      "       -0.00694255, -0.02852612], dtype=float32), array([-0.0784234 , -0.03567975, -0.01384762, ..., -0.039869  ,\n",
      "        0.07898825,  0.00345745], dtype=float32), array([-0.02693632, -0.03140344, -0.02048412, ...,  0.02268932,\n",
      "        0.06205763,  0.00993566], dtype=float32), array([-0.02342726, -0.01784593, -0.05326441, ..., -0.00431715,\n",
      "        0.02822074, -0.01705653], dtype=float32), array([-0.05189113, -0.0021922 , -0.01734922, ..., -0.03891531,\n",
      "        0.00068981, -0.00937664], dtype=float32), array([-0.01532753, -0.00075699, -0.03951575, ..., -0.00957837,\n",
      "       -0.01149502, -0.00510767], dtype=float32), array([-0.06935521,  0.00788613, -0.03095253, ...,  0.0264099 ,\n",
      "       -0.00125855, -0.01833991], dtype=float32), array([-0.04025086, -0.00778631, -0.02455173, ..., -0.00888218,\n",
      "        0.03959747, -0.01025615], dtype=float32), array([-0.02561158, -0.02114513, -0.00312908, ...,  0.01141691,\n",
      "        0.03461509,  0.00651775], dtype=float32), array([-0.04438302, -0.01854706, -0.01848283, ..., -0.01273599,\n",
      "       -0.03048843,  0.03011446], dtype=float32), array([-0.0584225 , -0.01287252, -0.02765886, ...,  0.00169025,\n",
      "       -0.01252928,  0.03509626], dtype=float32), array([-0.01230993, -0.0190931 , -0.06099399, ..., -0.0242756 ,\n",
      "        0.02916633,  0.01091058], dtype=float32), array([ 0.02017976, -0.03167585, -0.04253932, ...,  0.00421074,\n",
      "       -0.00132302, -0.00451465], dtype=float32), array([-0.00295345,  0.00298616, -0.02810308, ..., -0.01048958,\n",
      "        0.00956186, -0.02385432], dtype=float32), array([-0.06951389, -0.02060728, -0.01571382, ..., -0.00324703,\n",
      "       -0.0252298 ,  0.00509981], dtype=float32), array([-0.03772348,  0.00438861, -0.03398672, ..., -0.00736211,\n",
      "        0.02074002,  0.02168608], dtype=float32), array([-0.01116561, -0.01160244, -0.05978619, ..., -0.00808967,\n",
      "        0.01774736,  0.00350251], dtype=float32), array([-0.03590331, -0.03230758, -0.01237505, ..., -0.00516911,\n",
      "        0.02929556,  0.01890726], dtype=float32), array([-0.03805158, -0.01426225, -0.02315342, ..., -0.02920444,\n",
      "        0.03389061,  0.03957282], dtype=float32), array([-0.0069132 , -0.00700485, -0.0189249 , ..., -0.02096472,\n",
      "       -0.01836055,  0.00413201], dtype=float32), array([-0.04382546,  0.01438805, -0.02514028, ..., -0.00892684,\n",
      "       -0.01846479, -0.01312223], dtype=float32), array([-0.04673383, -0.03086163, -0.05683886, ..., -0.0253753 ,\n",
      "        0.04470235,  0.01388163], dtype=float32), array([-0.02867573, -0.02498657, -0.02278156, ..., -0.00860462,\n",
      "       -0.0063729 , -0.03674989], dtype=float32), array([-0.01905199, -0.01389978, -0.04864613, ...,  0.00603124,\n",
      "        0.00595705, -0.0227837 ], dtype=float32), array([-0.00160879, -0.00709524,  0.01302209, ..., -0.00688785,\n",
      "        0.04931378, -0.00853593], dtype=float32), array([-0.0190534 , -0.00958721, -0.02424363, ..., -0.00678432,\n",
      "       -0.02639236,  0.05712028], dtype=float32), array([-0.03702595,  0.01169555, -0.03222074, ...,  0.01228631,\n",
      "       -0.01277349, -0.03208717], dtype=float32), array([-0.01189244, -0.00512083,  0.00266171, ..., -0.02608427,\n",
      "        0.02882184,  0.00732792], dtype=float32), array([-0.01390387, -0.02402283, -0.03362978, ...,  0.00656818,\n",
      "       -0.00673243,  0.01114746], dtype=float32), array([-0.02077178, -0.0048857 , -0.00196865, ...,  0.01391863,\n",
      "       -0.02873418,  0.03174218], dtype=float32), array([-0.02651626, -0.01324667,  0.01315381, ...,  0.00019303,\n",
      "       -0.00745509, -0.02276766], dtype=float32), array([ 0.00266572,  0.00657108, -0.03415311, ...,  0.00470903,\n",
      "        0.01648827, -0.02878664], dtype=float32), array([-0.02645223, -0.02812391, -0.0091794 , ...,  0.00343891,\n",
      "       -0.02969895, -0.00975505], dtype=float32), array([-0.03582135, -0.03489989, -0.05641219, ...,  0.00909748,\n",
      "       -0.04335198,  0.00312791], dtype=float32), array([-0.0137186 ,  0.00376239, -0.02819435, ..., -0.00034445,\n",
      "        0.05353371, -0.00678948], dtype=float32), array([-0.00780933, -0.03661297, -0.03350967, ..., -0.00764745,\n",
      "        0.00733581,  0.02857105], dtype=float32), array([-0.01830194, -0.0282406 , -0.01625262, ..., -0.01181333,\n",
      "        0.01233369,  0.01919673], dtype=float32), array([-0.01750625, -0.02688931, -0.01723537, ..., -0.00134367,\n",
      "        0.00616451,  0.02571297], dtype=float32), array([-0.04405112, -0.03654887, -0.00719185, ..., -0.01305004,\n",
      "        0.01351068,  0.03088753], dtype=float32), array([-0.01985857,  0.0208077 , -0.01592278, ..., -0.00599919,\n",
      "        0.02553491, -0.00797891], dtype=float32), array([-0.01446982, -0.01764995, -0.0290039 , ..., -0.02746828,\n",
      "        0.04525382,  0.02648388], dtype=float32), array([-0.02916416, -0.02082667, -0.05351911, ...,  0.01568691,\n",
      "        0.0052558 ,  0.01550113], dtype=float32), array([-0.01180375, -0.00065707, -0.00200583, ..., -0.03335265,\n",
      "        0.00323543, -0.01799394], dtype=float32), array([-0.04426038, -0.00585488, -0.00227689, ...,  0.02991603,\n",
      "        0.02167734,  0.02718855], dtype=float32), array([-0.01522496, -0.00851392, -0.03428032, ..., -0.01663305,\n",
      "        0.02101082, -0.00752658], dtype=float32), array([-0.0241219 ,  0.01521463, -0.01220414, ..., -0.01468617,\n",
      "        0.04309199,  0.00555482], dtype=float32), array([-0.05093389,  0.00399806, -0.02357644, ..., -0.02346814,\n",
      "       -0.00745537,  0.00761569], dtype=float32), array([-0.04371981, -0.01030848, -0.0259129 , ..., -0.00210001,\n",
      "        0.01117789, -0.00734099], dtype=float32), array([-0.02315862,  0.00518571, -0.01266898, ..., -0.02152022,\n",
      "        0.03576174,  0.00218344], dtype=float32), array([-0.03979266, -0.01460887, -0.03752362, ...,  0.014715  ,\n",
      "        0.00419272,  0.03191724], dtype=float32), array([-0.0630827 ,  0.01507254, -0.03957026, ..., -0.01415816,\n",
      "        0.03084366, -0.01469345], dtype=float32), array([-0.03343273, -0.03820945, -0.06067218, ..., -0.01545233,\n",
      "       -0.01643869,  0.00085012], dtype=float32), array([-0.03143756, -0.01300418, -0.02056524, ..., -0.00535808,\n",
      "        0.01049465, -0.00936989], dtype=float32), array([-0.06626774,  0.00557156, -0.01248846, ..., -0.02876533,\n",
      "        0.01036139,  0.01192631], dtype=float32), array([-0.02469402, -0.05353761, -0.03495523, ..., -0.00199215,\n",
      "       -0.06337069,  0.03695184], dtype=float32), array([-0.07744341, -0.02298281, -0.03197869, ..., -0.05478427,\n",
      "       -0.00178818, -0.00649665], dtype=float32), array([-0.02906889, -0.01702234,  0.00460715, ...,  0.00930216,\n",
      "        0.01881932,  0.04682702], dtype=float32)], 'sparse': <Compressed Sparse Row sparse array of dtype 'float64'\n",
      "\twith 12170 stored elements and shape (60, 250002)>}\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings using BGE-M3 model\n",
    "docs_embeddings = dense_embedding_function(all_content_list)\n",
    "# docs_embeddings = ef.encode_documents(docs)\n",
    "print(docs_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through the text lines, create embeddings, and then insert the data into Milvus.\n",
    "- Here is a new field text, which is a non-defined field in the collection schema. It will be automatically added to the reserved JSON dynamic field, which can be treated as a normal field at a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities inserted: 60\n"
     ]
    }
   ],
   "source": [
    "# Insert the embeddings into the Milvus collection\n",
    "for i in range(0, len(all_content_list), 50): # Batch the embeddings to insert into Milvus to avoid memory issues in sets of 50\n",
    "    batched_entities = [\n",
    "        docs_embeddings[\"dense\"][i : i + 50], # Dense embeddings\n",
    "        docs_embeddings[\"sparse\"][i : i + 50], # Sparse embeddings\n",
    "        all_content_list[i : i + 50], # Raw content in text format\n",
    "    ]\n",
    "    milvus_hybrid_collection.insert(batched_entities)\n",
    "print(\"Number of entities inserted:\", milvus_hybrid_collection.num_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'insert_count': 60, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59], 'cost': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, line in enumerate(tqdm(all_content_list, desc=\"Creating embeddings\")):\n",
    "    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n",
    "\n",
    "milvus_lite_client.client.insert(collection_name='my_rag_collection', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a simple RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which vector database should I use?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the question in the collection and retrieve the semantic top-3 matches\n",
    "search_res = milvus_lite_client.client.search(\n",
    "    collection_name='my_rag_collection',\n",
    "    data=[\n",
    "        emb_text(question)\n",
    "    ],  # Use the `emb_text` function to convert the question to an embedding vector\n",
    "    limit=3,  # Return top 3 results\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
    "    output_fields=[\"text\"],  # Return the text field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"\\n<START Article Number: 40>\\nTitle: Binary vector embeddings are so cool\\nURL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\nSummary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings\\u2014which prioritize important information at the beginning of the vector\\u2014further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \\n\\n- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\\n- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\\n- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\\n- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\\nDescription: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression \\ud83e\\udd2f.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 40>\\n\",\n",
      "        0.4335808753967285\n",
      "    ],\n",
      "    [\n",
      "        \"\\n<START Article Number: 21>\\nTitle: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\\nURL: https://github.com/pingcap/autoflow\\nSummary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\\n\\n- Utilize Graph RAG architecture for enhanced conversational AI.\\n- Implement a built-in website crawler for improved documentation search.\\n- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\\n- Leverage TiDB for efficient data management and storage.\\n- Contribute to the project by following community guidelines.\\nDescription: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\\nCreated: 2024-11-22\\nType: web-page\\n<END Article Number: 21>\\n\",\n",
      "        0.3465275764465332\n",
      "    ],\n",
      "    [\n",
      "        \"\\n<START Article Number: 39>\\nTitle: Comparing full text search algorithms: BM25, TF-IDF, and Postgres\\nURL: https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\\nSummary: The comparison of full text search algorithms BM25, TF-IDF, and PostgreSQL's full text search highlights significant differences in their approaches to document relevance scoring. BM25 improves upon TF-IDF by incorporating a saturation function for term frequency, document length normalization, and a smoothed Inverse Document Frequency (IDF), which collectively enhance its ability to rank documents more effectively. In contrast, TF-IDF lacks these features, relying on simpler heuristics. When compared to PostgreSQL's full text search, BM25 offers a more sophisticated ranking mechanism that considers term rarity and document length, while PostgreSQL primarily uses stopword dictionaries and basic term frequency without saturation. The introduction of the `pg_bm25` extension for ParadeDB demonstrates the growing need for advanced search capabilities within PostgreSQL environments. Overall, while BM25 provides superior search quality, PostgreSQL's simplicity may appeal to certain applications. \\n\\n### Key Points:\\n- **BM25 vs TF-IDF:** BM25 introduces saturation for term frequency, document length normalization, and smoothed IDF, enhancing relevance scoring.\\n- **PostgreSQL Limitations:** PostgreSQL's full text search lacks the sophistication of BM25, relying on stopword dictionaries and basic term frequency.\\n- **Performance Considerations:** ParadeDB's `pg_bm25` extension offers BM25-based ranking, significantly improving search speed compared to native PostgreSQL.\\n- **Application Context:** Choose BM25 for applications requiring high search quality; opt for PostgreSQL for simpler implementations.\\nDescription: I wrote another post about  and had initially included comparisons with two other algorithms. However, that post was already quite long so here are the brief...\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 39>\\n\",\n",
      "        0.34186169505119324\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Print the retrieved lines with distances\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dense': [array([-0.03993049, -0.02827392, -0.05900438, ...,  0.01967705,\n",
       "          0.00615428,  0.00818304], dtype=float32)],\n",
       " 'sparse': <Compressed Sparse Row sparse array of dtype 'float64'\n",
       " \twith 8 stored elements and shape (1, 250002)>}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate embeddings for the query\n",
    "query_embeddings = dense_embedding_function([question])\n",
    "query_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense': [\"\\n<START Article Number: 41>\\nTitle: Understanding the BM25 full text search algorithm\\nURL: https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\\nSummary: BM25 (Best Match 25) is a prominent full-text search algorithm utilized in systems like Lucene, Elasticsearch, and SQLite, known for its effectiveness in ranking documents based on their relevance to a query. It operates on the principle of probabilistic ranking, leveraging components such as Inverse Document Frequency (IDF), term frequency, and document length normalization to compute scores for documents. The algorithm's clever design allows it to rank documents without needing to calculate exact probabilities, making it practical for real-world applications. Notably, BM25 scores can only be compared within the same document collection, as they depend on the specific characteristics of that collection. This understanding is crucial for developers looking to implement or enhance search functionalities in applications, particularly when integrating full-text search with vector similarity search for improved content retrieval.\\n\\n### Key Points:\\n- **BM25 Algorithm**: A widely adopted full-text search algorithm that ranks documents based on relevance.\\n- **Components**: Utilizes query terms, IDF, term frequency, and document length normalization.\\n- **Probabilistic Ranking**: Focuses on the order of documents rather than exact relevance probabilities.\\n- **Score Comparisons**: BM25 scores can only be compared within the same document collection due to dependency on collection-specific metrics.\\n- **Hybrid Search**: Increasingly used in conjunction with vector similarity search to enhance search capabilities.\\nDescription: BM25 is a widely used algorithm for full text search. I wanted to understand how it works, so here is my attempt at understanding by re-explaining.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 41>\\n\",\n",
       "  '\\n<START Article Number: 40>\\nTitle: Binary vector embeddings are so cool\\nURL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\nSummary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \\n\\n- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\\n- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\\n- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\\n- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\\nDescription: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 40>\\n',\n",
       "  '\\n<START Article Number: 21>\\nTitle: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\\nURL: https://github.com/pingcap/autoflow\\nSummary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\\n\\n- Utilize Graph RAG architecture for enhanced conversational AI.\\n- Implement a built-in website crawler for improved documentation search.\\n- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\\n- Leverage TiDB for efficient data management and storage.\\n- Contribute to the project by following community guidelines.\\nDescription: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\\nCreated: 2024-11-22\\nType: web-page\\n<END Article Number: 21>\\n'],\n",
       " 'sparse': ['\\n<START Article Number: 40>\\nTitle: Binary vector embeddings are so cool\\nURL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\nSummary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \\n\\n- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\\n- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\\n- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\\n- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\\nDescription: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 40>\\n',\n",
       "  '\\n<START Article Number: 58>\\nTitle: voyage-multimodal-3: all-in-one embedding model for interleaved text, images, and screenshots\\nURL: https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/\\nSummary: Voyage AI has launched `voyage-multimodal-3`, a cutting-edge multimodal embedding model that integrates interleaved text and images, significantly enhancing retrieval accuracy for documents containing both visual and textual data. This model outperforms existing solutions like OpenAI CLIP and Cohere multimodal v3 by an average of 19.63% across various multimodal retrieval tasks, including table/figure retrieval and document screenshot retrieval. Unlike traditional models that process text and images separately, `voyage-multimodal-3` utilizes a unified transformer architecture, allowing for more effective vectorization of complex layouts without the need for heuristic parsing. This innovation addresses the modality gap issue prevalent in CLIP-like models, ensuring robust performance in mixed-modality searches. The model is evaluated across 20 multimodal datasets and demonstrates superior capabilities in capturing semantic content from screenshots and documents, making it a valuable tool for semantic search and retrieval-augmented generation (RAG) applications.\\n\\n### Key Points:\\n- `voyage-multimodal-3` integrates text and image data for improved retrieval accuracy.\\n- Achieves an average of 19.63% better performance than leading multimodal models.\\n- Utilizes a unified transformer architecture for vectorization, enhancing flexibility and accuracy.\\n- Addresses the modality gap, improving mixed-modality search results.\\n- Evaluated across 20 multimodal datasets, showcasing superior capabilities in semantic content retrieval.\\nDescription: TL;DR — We are excited to announce voyage-multimodal-3, a new state-of-the-art for multimodal embeddings and a big step forward towards seamless RAG and semantic search for documents rich with both…\\nCreated: 2024-11-18\\nType: web-page\\n<END Article Number: 58>\\n',\n",
       "  '\\n<START Article Number: 17>\\nTitle: The AI agents stack  | Letta\\nURL: https://www.letta.com/blog/ai-agents-stack\\nSummary: The AI agents stack has evolved significantly, reflecting advancements in memory, tool usage, and deployment strategies. This stack is categorized into three layers: model serving, storage, and agent frameworks. The transition from LLMs to LLM agents highlights the complexity of state management and tool execution, which are critical for developing autonomous systems. Key players in model serving include OpenAI and Anthropic for closed APIs, while vLLM and Ollama cater to local inference needs. Storage solutions like Chroma and Pinecone support the stateful nature of agents, enabling them to retain conversation histories and external data. The ability to call tools through structured outputs distinguishes agents from traditional chatbots, necessitating secure execution environments. Frameworks like Letta and LangChain manage agent state and context, with varying approaches to memory management and cross-agent communication. The future of agent deployment is anticipated to shift towards service-oriented architectures, emphasizing REST APIs for scalability and state normalization. As the ecosystem matures, the choice of frameworks will become increasingly critical for developers building complex agent applications.\\n\\n- Understand the three layers of the AI agents stack: model serving, storage, and agent frameworks.\\n- Recognize the importance of state management and tool execution in developing LLM agents.\\n- Explore model serving options, including both closed APIs and local inference solutions.\\n- Utilize vector databases for effective storage of agent state and conversation history.\\n- Implement secure execution environments for tool calls made by agents.\\n- Choose frameworks based on their state management, memory handling, and support for open models.\\n- Prepare for a shift towards service-oriented architectures for agent deployment, focusing on REST APIs.\\nDescription: Understanding the AI agents stack landscape.\\nCreated: 2024-11-23\\nType: web-page\\n<END Article Number: 17>\\n'],\n",
       " 'hybrid': ['\\n<START Article Number: 40>\\nTitle: Binary vector embeddings are so cool\\nURL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\nSummary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \\n\\n- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\\n- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\\n- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\\n- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\\nDescription: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 40>\\n',\n",
       "  \"\\n<START Article Number: 41>\\nTitle: Understanding the BM25 full text search algorithm\\nURL: https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\\nSummary: BM25 (Best Match 25) is a prominent full-text search algorithm utilized in systems like Lucene, Elasticsearch, and SQLite, known for its effectiveness in ranking documents based on their relevance to a query. It operates on the principle of probabilistic ranking, leveraging components such as Inverse Document Frequency (IDF), term frequency, and document length normalization to compute scores for documents. The algorithm's clever design allows it to rank documents without needing to calculate exact probabilities, making it practical for real-world applications. Notably, BM25 scores can only be compared within the same document collection, as they depend on the specific characteristics of that collection. This understanding is crucial for developers looking to implement or enhance search functionalities in applications, particularly when integrating full-text search with vector similarity search for improved content retrieval.\\n\\n### Key Points:\\n- **BM25 Algorithm**: A widely adopted full-text search algorithm that ranks documents based on relevance.\\n- **Components**: Utilizes query terms, IDF, term frequency, and document length normalization.\\n- **Probabilistic Ranking**: Focuses on the order of documents rather than exact relevance probabilities.\\n- **Score Comparisons**: BM25 scores can only be compared within the same document collection due to dependency on collection-specific metrics.\\n- **Hybrid Search**: Increasingly used in conjunction with vector similarity search to enhance search capabilities.\\nDescription: BM25 is a widely used algorithm for full text search. I wanted to understand how it works, so here is my attempt at understanding by re-explaining.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 41>\\n\",\n",
       "  '\\n<START Article Number: 21>\\nTitle: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\\nURL: https://github.com/pingcap/autoflow\\nSummary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\\n\\n- Utilize Graph RAG architecture for enhanced conversational AI.\\n- Implement a built-in website crawler for improved documentation search.\\n- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\\n- Leverage TiDB for efficient data management and storage.\\n- Contribute to the project by following community guidelines.\\nDescription: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\\nCreated: 2024-11-22\\nType: web-page\\n<END Article Number: 21>\\n']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_results = milvus_full_client.dense_search(milvus_hybrid_collection, query_embeddings[\"dense\"][0], limit=3)\n",
    "sparse_results = milvus_full_client.sparse_search(milvus_hybrid_collection, query_embeddings[\"sparse\"]._getrow(0), limit=3)\n",
    "hybrid_results = milvus_full_client.hybrid_search(\n",
    "    milvus_hybrid_collection,\n",
    "    query_embeddings[\"dense\"][0],\n",
    "    query_embeddings[\"sparse\"]._getrow(0),\n",
    "    sparse_weight=0.7,\n",
    "    dense_weight=1.0,\n",
    "    limit=3,\n",
    ")\n",
    "\n",
    "results_dict = {\n",
    "    \"dense\": dense_results,\n",
    "    \"sparse\": sparse_results,\n",
    "    \"hybrid\": hybrid_results,\n",
    "}\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_text_formatting(ef, query, docs):\n",
    "    tokenizer = ef.model.tokenizer\n",
    "    query_tokens_ids = tokenizer.encode(query, return_offsets_mapping=True)\n",
    "    query_tokens = tokenizer.convert_ids_to_tokens(query_tokens_ids)\n",
    "    formatted_texts = []\n",
    "\n",
    "    for doc in docs:\n",
    "        ldx = 0\n",
    "        landmarks = []\n",
    "        encoding = tokenizer.encode_plus(doc, return_offsets_mapping=True)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"])[1:-1]\n",
    "        offsets = encoding[\"offset_mapping\"][1:-1]\n",
    "        for token, (start, end) in zip(tokens, offsets):\n",
    "            if token in query_tokens:\n",
    "                if len(landmarks) != 0 and start == landmarks[-1]:\n",
    "                    landmarks[-1] = end\n",
    "                else:\n",
    "                    landmarks.append(start)\n",
    "                    landmarks.append(end)\n",
    "        close = False\n",
    "        formatted_text = \"\"\n",
    "        for i, c in enumerate(doc):\n",
    "            if ldx == len(landmarks):\n",
    "                pass\n",
    "            elif i == landmarks[ldx]:\n",
    "                if close:\n",
    "                    formatted_text += \"</span>\"\n",
    "                else:\n",
    "                    formatted_text += \"<span style='color:red'>\"\n",
    "                close = not close\n",
    "                ldx = ldx + 1\n",
    "            formatted_text += c\n",
    "        if close is True:\n",
    "            formatted_text += \"</span>\"\n",
    "        formatted_texts.append(formatted_text)\n",
    "    return formatted_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Dense Search Results:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 41>\n",
       "Title: Understanding the BM25 full text search algorithm\n",
       "URL: https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\n",
       "Summary: BM25 (Best Match 25) is a prominent full-text search algorithm utilized in systems like Lucene, Elasticsearch, and SQLite, known for its effectiveness in ranking documents based on their relevance to a query. It operates on the principle of probabilistic ranking, leveraging components such as Inverse Document Frequency (IDF), term frequency, and document length normalization to compute scores for documents. The algorithm's clever design allows it to rank documents without needing to calculate exact probabilities, making it practical for real-world applications. Notably, BM25 scores can only be compared within the same document collection, as they depend on the specific characteristics of that collection. This understanding is crucial for developers looking to implement or enhance search functionalities in applications, particularly when integrating full-text search with<span style='color:red'> vector</span> similarity search for improved content retrieval.\n",
       "\n",
       "### Key Points:\n",
       "- **BM25 Algorithm**: A widely adopted full-text search algorithm that ranks documents based on relevance.\n",
       "- **Components**: Utilizes query terms,<span style='color:red'> I</span>DF, term frequency, and document length normalization.\n",
       "- **Probabilistic Ranking**: Focuses on the order of documents rather than exact relevance probabilities.\n",
       "- **Score Comparisons**: BM25 scores can only be compared within the same document collection due to dependency on collection-specific metrics.\n",
       "- **Hybrid Search**: Increasingly used in conjunction with<span style='color:red'> vector</span> similarity search to enhance search capabilities.\n",
       "Description: BM25 is a widely used algorithm for full text search.<span style='color:red'> I</span> wanted to understand how it works, so here is my attempt at understanding by re-explaining.\n",
       "Created: 2024-11-20\n",
       "Type: web-page\n",
       "<END Article Number: 41>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 40>\n",
       "Title: Binary<span style='color:red'> vector</span> embeddings are so cool\n",
       "URL: https://emschwartz.me/binary-ve<span style='color:red'>ctor</span>-embeddings-are-so-cool/\n",
       "Summary: Binary quantized<span style='color:red'> vector</span> embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the<span style='color:red'> vector</span>—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate<span style='color:red'> vector</span> similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
       "\n",
       "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
       "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
       "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
       "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
       "Description: Ve<span style='color:red'>ctor</span> embeddings by themselves are pretty neat. Binary quantized<span style='color:red'> vector</span> embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
       "Created: 2024-11-20\n",
       "Type: web-page\n",
       "<END Article Number: 40>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 21>\n",
       "Title: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Ve<span style='color:red'>ctor</span> Storage. Demo: https://tidb.ai\n",
       "URL: https://github.com/pingcap/autoflow\n",
       "Summary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Ve<span style='color:red'>ctor</span> Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\n",
       "\n",
       "- Utilize Graph RAG architecture for enhanced conversational AI.\n",
       "- Implement a built-in website crawler for improved documentation search.\n",
       "- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\n",
       "- Leverage TiDB for efficient data management and storage.\n",
       "- Contribute to the project by following community guidelines.\n",
       "Description: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Ve<span style='color:red'>ctor</span> Storage. Demo: https://tidb.ai - pingcap/autoflow\n",
       "Created: 2024-11-22\n",
       "Type: web-page\n",
       "<END Article Number: 21>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Sparse Search Results:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 40>\n",
       "Title: Binary<span style='color:red'> vector</span> embeddings are so cool\n",
       "URL: https://emschwartz.me/binary-ve<span style='color:red'>ctor</span>-embeddings-are-so-cool/\n",
       "Summary: Binary quantized<span style='color:red'> vector</span> embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the<span style='color:red'> vector</span>—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate<span style='color:red'> vector</span> similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
       "\n",
       "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
       "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
       "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
       "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
       "Description: Ve<span style='color:red'>ctor</span> embeddings by themselves are pretty neat. Binary quantized<span style='color:red'> vector</span> embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
       "Created: 2024-11-20\n",
       "Type: web-page\n",
       "<END Article Number: 40>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 58>\n",
       "Title: voyage-multimodal-3: all-in-one embedding model for interleaved text, images, and screenshots\n",
       "URL: https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/\n",
       "Summary: Voyage AI has launched `voyage-multimodal-3`, a cutting-edge multimodal embedding model that integrates interleaved text and images, significantly enhancing retrieval accuracy for documents containing both visual and textual data. This model outperforms existing solutions like OpenAI CLIP and Cohere multimodal v3 by an average of 19.63% across various multimodal retrieval tasks, including table/figure retrieval and document screenshot retrieval. Unlike traditional models that process text and images separately, `voyage-multimodal-3` utilizes a unified transformer architecture, allowing for more effective<span style='color:red'> vector</span>ization of complex layouts without the need for heuristic parsing. This innovation addresses the modality gap issue prevalent in CLIP-like models, ensuring robust performance in mixed-modality searches. The model is evaluated across 20 multimodal datasets and demonstrates superior capabilities in capturing semantic content from screenshots and documents, making it a valuable tool for semantic search and retrieval-augmented generation (RAG) applications.\n",
       "\n",
       "### Key Points:\n",
       "- `voyage-multimodal-3` integrates text and image data for improved retrieval accuracy.\n",
       "- Achieves an average of 19.63% better performance than leading multimodal models.\n",
       "- Utilizes a unified transformer architecture for<span style='color:red'> vector</span>ization, enhancing flexibility and accuracy.\n",
       "- Addresses the modality gap, improving mixed-modality search results.\n",
       "- Evaluated across 20 multimodal datasets, showcasing superior capabilities in semantic content retrieval.\n",
       "Description: TL;DR — We are excited to announce voyage-multimodal-3, a new state-of-the-art for multimodal embeddings and a big step forward towards seamless RAG and semantic search for documents rich with both…\n",
       "Created: 2024-11-18\n",
       "Type: web-page\n",
       "<END Article Number: 58>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 17>\n",
       "Title: The AI agents stack  | Letta\n",
       "URL: https://www.letta.com/blog/ai-agents-stack\n",
       "Summary: The AI agents stack has evolved significantly, reflecting advancements in memory, tool usage, and deployment strategies. This stack is categorized into three layers: model serving, storage, and agent frameworks. The transition from LLMs to LLM agents highlights the complexity of state management and tool execution, which are critical for developing autonomous systems. Key players in model serving include OpenAI and Anthropic for closed APIs, while vLLM and Ollama cater to local inference needs. Storage solutions like Chroma and Pinecone support the stateful nature of agents, enabling them to retain conversation histories and external data. The ability to call tools through structured outputs distinguishes agents from traditional chatbots, necessitating secure execution environments. Frameworks like Letta and LangChain manage agent state and context, with varying approaches to memory management and cross-agent communication. The future of agent deployment is anticipated to shift towards service-oriented architectures, emphasizing REST APIs for scalability and state normalization. As the ecosystem matures, the choice of frameworks will become increasingly critical for developers building complex agent applications.\n",
       "\n",
       "- Understand the three layers of the AI agents stack: model serving, storage, and agent frameworks.\n",
       "- Recognize the importance of state management and tool execution in developing LLM agents.\n",
       "- Explore model serving options, including both closed APIs and local inference solutions.\n",
       "- Utilize<span style='color:red'> vector database</span>s for effective storage of agent state and conversation history.\n",
       "- Implement secure execution environments for tool calls made by agents.\n",
       "- Choose frameworks based on their state management, memory handling, and support for open models.\n",
       "- Prepare for a shift towards service-oriented architectures for agent deployment, focusing on REST APIs.\n",
       "Description: Understanding the AI agents stack landscape.\n",
       "Created: 2024-11-23\n",
       "Type: web-page\n",
       "<END Article Number: 17>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "**Hybrid Search Results:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 40>\n",
       "Title: Binary<span style='color:red'> vector</span> embeddings are so cool\n",
       "URL: https://emschwartz.me/binary-ve<span style='color:red'>ctor</span>-embeddings-are-so-cool/\n",
       "Summary: Binary quantized<span style='color:red'> vector</span> embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the<span style='color:red'> vector</span>—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate<span style='color:red'> vector</span> similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
       "\n",
       "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
       "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
       "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
       "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
       "Description: Ve<span style='color:red'>ctor</span> embeddings by themselves are pretty neat. Binary quantized<span style='color:red'> vector</span> embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
       "Created: 2024-11-20\n",
       "Type: web-page\n",
       "<END Article Number: 40>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 41>\n",
       "Title: Understanding the BM25 full text search algorithm\n",
       "URL: https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\n",
       "Summary: BM25 (Best Match 25) is a prominent full-text search algorithm utilized in systems like Lucene, Elasticsearch, and SQLite, known for its effectiveness in ranking documents based on their relevance to a query. It operates on the principle of probabilistic ranking, leveraging components such as Inverse Document Frequency (IDF), term frequency, and document length normalization to compute scores for documents. The algorithm's clever design allows it to rank documents without needing to calculate exact probabilities, making it practical for real-world applications. Notably, BM25 scores can only be compared within the same document collection, as they depend on the specific characteristics of that collection. This understanding is crucial for developers looking to implement or enhance search functionalities in applications, particularly when integrating full-text search with<span style='color:red'> vector</span> similarity search for improved content retrieval.\n",
       "\n",
       "### Key Points:\n",
       "- **BM25 Algorithm**: A widely adopted full-text search algorithm that ranks documents based on relevance.\n",
       "- **Components**: Utilizes query terms,<span style='color:red'> I</span>DF, term frequency, and document length normalization.\n",
       "- **Probabilistic Ranking**: Focuses on the order of documents rather than exact relevance probabilities.\n",
       "- **Score Comparisons**: BM25 scores can only be compared within the same document collection due to dependency on collection-specific metrics.\n",
       "- **Hybrid Search**: Increasingly used in conjunction with<span style='color:red'> vector</span> similarity search to enhance search capabilities.\n",
       "Description: BM25 is a widely used algorithm for full text search.<span style='color:red'> I</span> wanted to understand how it works, so here is my attempt at understanding by re-explaining.\n",
       "Created: 2024-11-20\n",
       "Type: web-page\n",
       "<END Article Number: 41>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "<START Article Number: 21>\n",
       "Title: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Ve<span style='color:red'>ctor</span> Storage. Demo: https://tidb.ai\n",
       "URL: https://github.com/pingcap/autoflow\n",
       "Summary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Ve<span style='color:red'>ctor</span> Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\n",
       "\n",
       "- Utilize Graph RAG architecture for enhanced conversational AI.\n",
       "- Implement a built-in website crawler for improved documentation search.\n",
       "- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\n",
       "- Leverage TiDB for efficient data management and storage.\n",
       "- Contribute to the project by following community guidelines.\n",
       "Description: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Ve<span style='color:red'>ctor</span> Storage. Demo: https://tidb.ai - pingcap/autoflow\n",
       "Created: 2024-11-22\n",
       "Type: web-page\n",
       "<END Article Number: 21>\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(\"**Dense Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(dense_embedding_function, question, dense_results)\n",
    "for result in formatted_results:\n",
    "    display(Markdown(result))\n",
    "\n",
    "display(Markdown(\"\\n**Sparse Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(dense_embedding_function, question, sparse_results)\n",
    "for result in formatted_results:\n",
    "    display(Markdown(result))\n",
    "\n",
    "display(Markdown(\"\\n**Hybrid Search Results:**\"))\n",
    "formatted_results = doc_text_formatting(dense_embedding_function, question, hybrid_results)\n",
    "for result in formatted_results:\n",
    "    display(Markdown(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a LLM to answer the question using the retrieved lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<START Article Number: 40>\n",
      "Title: Binary vector embeddings are so cool\n",
      "URL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\n",
      "Summary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
      "\n",
      "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
      "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
      "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
      "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
      "Description: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 40>\n",
      "\n",
      "\n",
      "<START Article Number: 21>\n",
      "Title: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\n",
      "URL: https://github.com/pingcap/autoflow\n",
      "Summary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\n",
      "\n",
      "- Utilize Graph RAG architecture for enhanced conversational AI.\n",
      "- Implement a built-in website crawler for improved documentation search.\n",
      "- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\n",
      "- Leverage TiDB for efficient data management and storage.\n",
      "- Contribute to the project by following community guidelines.\n",
      "Description: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\n",
      "Created: 2024-11-22\n",
      "Type: web-page\n",
      "<END Article Number: 21>\n",
      "\n",
      "\n",
      "<START Article Number: 39>\n",
      "Title: Comparing full text search algorithms: BM25, TF-IDF, and Postgres\n",
      "URL: https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\n",
      "Summary: The comparison of full text search algorithms BM25, TF-IDF, and PostgreSQL's full text search highlights significant differences in their approaches to document relevance scoring. BM25 improves upon TF-IDF by incorporating a saturation function for term frequency, document length normalization, and a smoothed Inverse Document Frequency (IDF), which collectively enhance its ability to rank documents more effectively. In contrast, TF-IDF lacks these features, relying on simpler heuristics. When compared to PostgreSQL's full text search, BM25 offers a more sophisticated ranking mechanism that considers term rarity and document length, while PostgreSQL primarily uses stopword dictionaries and basic term frequency without saturation. The introduction of the `pg_bm25` extension for ParadeDB demonstrates the growing need for advanced search capabilities within PostgreSQL environments. Overall, while BM25 provides superior search quality, PostgreSQL's simplicity may appeal to certain applications. \n",
      "\n",
      "### Key Points:\n",
      "- **BM25 vs TF-IDF:** BM25 introduces saturation for term frequency, document length normalization, and smoothed IDF, enhancing relevance scoring.\n",
      "- **PostgreSQL Limitations:** PostgreSQL's full text search lacks the sophistication of BM25, relying on stopword dictionaries and basic term frequency.\n",
      "- **Performance Considerations:** ParadeDB's `pg_bm25` extension offers BM25-based ranking, significantly improving search speed compared to native PostgreSQL.\n",
      "- **Application Context:** Choose BM25 for applications requiring high search quality; opt for PostgreSQL for simpler implementations.\n",
      "Description: I wrote another post about  and had initially included comparisons with two other algorithms. However, that post was already quite long so here are the brief...\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 39>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the retrieved lines with a newline character\n",
    "basic_context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")\n",
    "print(basic_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<START Article Number: 40>\n",
      "Title: Binary vector embeddings are so cool\n",
      "URL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\n",
      "Summary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
      "\n",
      "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
      "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
      "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
      "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
      "Description: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 40>\n",
      "\n",
      "\n",
      "<START Article Number: 41>\n",
      "Title: Understanding the BM25 full text search algorithm\n",
      "URL: https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\n",
      "Summary: BM25 (Best Match 25) is a prominent full-text search algorithm utilized in systems like Lucene, Elasticsearch, and SQLite, known for its effectiveness in ranking documents based on their relevance to a query. It operates on the principle of probabilistic ranking, leveraging components such as Inverse Document Frequency (IDF), term frequency, and document length normalization to compute scores for documents. The algorithm's clever design allows it to rank documents without needing to calculate exact probabilities, making it practical for real-world applications. Notably, BM25 scores can only be compared within the same document collection, as they depend on the specific characteristics of that collection. This understanding is crucial for developers looking to implement or enhance search functionalities in applications, particularly when integrating full-text search with vector similarity search for improved content retrieval.\n",
      "\n",
      "### Key Points:\n",
      "- **BM25 Algorithm**: A widely adopted full-text search algorithm that ranks documents based on relevance.\n",
      "- **Components**: Utilizes query terms, IDF, term frequency, and document length normalization.\n",
      "- **Probabilistic Ranking**: Focuses on the order of documents rather than exact relevance probabilities.\n",
      "- **Score Comparisons**: BM25 scores can only be compared within the same document collection due to dependency on collection-specific metrics.\n",
      "- **Hybrid Search**: Increasingly used in conjunction with vector similarity search to enhance search capabilities.\n",
      "Description: BM25 is a widely used algorithm for full text search. I wanted to understand how it works, so here is my attempt at understanding by re-explaining.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 41>\n",
      "\n",
      "\n",
      "<START Article Number: 21>\n",
      "Title: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\n",
      "URL: https://github.com/pingcap/autoflow\n",
      "Summary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\n",
      "\n",
      "- Utilize Graph RAG architecture for enhanced conversational AI.\n",
      "- Implement a built-in website crawler for improved documentation search.\n",
      "- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\n",
      "- Leverage TiDB for efficient data management and storage.\n",
      "- Contribute to the project by following community guidelines.\n",
      "Description: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\n",
      "Created: 2024-11-22\n",
      "Type: web-page\n",
      "<END Article Number: 21>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hybrid_context = \"\\n\".join(\n",
    "    [hybrid_match for hybrid_match in hybrid_results]\n",
    ")\n",
    "print(hybrid_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_template(context, question):\n",
    "    SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\"\"\"\n",
    "    USER_PROMPT = \"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\"\"\".format(context=context, question=question)\n",
    "    return SYSTEM_PROMPT, USER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, you might consider using TiDB Serverless Vector Storage if you're looking for a vector database that supports conversational AI and advanced features like a built-in website crawler and a flexible architecture (Graph RAG). This database is particularly suited for developers aiming to implement conversational knowledge bases and enhance user interaction through intelligent search capabilities. Alternatively, if you're focused solely on the performance of vector embeddings, particularly in natural language processing tasks, you may want to explore binary quantized vector embeddings due to their efficiency, high retrieval accuracy, and significant speed improvements for similarity searches.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT, USER_PROMPT = prompt_template(basic_context, question)\n",
    "basic_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "print(basic_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which vector database should I use?\n",
      "Basic context derived from the vector database: \n",
      "<START Article Number: 40>\n",
      "Title: Binary vector embeddings are so cool\n",
      "URL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\n",
      "Summary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
      "\n",
      "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
      "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
      "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
      "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
      "Description: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 40>\n",
      "\n",
      "\n",
      "<START Article Number: 21>\n",
      "Title: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\n",
      "URL: https://github.com/pingcap/autoflow\n",
      "Summary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\n",
      "\n",
      "- Utilize Graph RAG architecture for enhanced conversational AI.\n",
      "- Implement a built-in website crawler for improved documentation search.\n",
      "- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\n",
      "- Leverage TiDB for efficient data management and storage.\n",
      "- Contribute to the project by following community guidelines.\n",
      "Description: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\n",
      "Created: 2024-11-22\n",
      "Type: web-page\n",
      "<END Article Number: 21>\n",
      "\n",
      "\n",
      "<START Article Number: 39>\n",
      "Title: Comparing full text search algorithms: BM25, TF-IDF, and Postgres\n",
      "URL: https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\n",
      "Summary: The comparison of full text search algorithms BM25, TF-IDF, and PostgreSQL's full text search highlights significant differences in their approaches to document relevance scoring. BM25 improves upon TF-IDF by incorporating a saturation function for term frequency, document length normalization, and a smoothed Inverse Document Frequency (IDF), which collectively enhance its ability to rank documents more effectively. In contrast, TF-IDF lacks these features, relying on simpler heuristics. When compared to PostgreSQL's full text search, BM25 offers a more sophisticated ranking mechanism that considers term rarity and document length, while PostgreSQL primarily uses stopword dictionaries and basic term frequency without saturation. The introduction of the `pg_bm25` extension for ParadeDB demonstrates the growing need for advanced search capabilities within PostgreSQL environments. Overall, while BM25 provides superior search quality, PostgreSQL's simplicity may appeal to certain applications. \n",
      "\n",
      "### Key Points:\n",
      "- **BM25 vs TF-IDF:** BM25 introduces saturation for term frequency, document length normalization, and smoothed IDF, enhancing relevance scoring.\n",
      "- **PostgreSQL Limitations:** PostgreSQL's full text search lacks the sophistication of BM25, relying on stopword dictionaries and basic term frequency.\n",
      "- **Performance Considerations:** ParadeDB's `pg_bm25` extension offers BM25-based ranking, significantly improving search speed compared to native PostgreSQL.\n",
      "- **Application Context:** Choose BM25 for applications requiring high search quality; opt for PostgreSQL for simpler implementations.\n",
      "Description: I wrote another post about  and had initially included comparisons with two other algorithms. However, that post was already quite long so here are the brief...\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 39>\n",
      "\n",
      "Answer: Based on the provided information, you might consider using TiDB Serverless Vector Storage if you're looking for a vector database that supports conversational AI and advanced features like a built-in website crawler and a flexible architecture (Graph RAG). This database is particularly suited for developers aiming to implement conversational knowledge bases and enhance user interaction through intelligent search capabilities. Alternatively, if you're focused solely on the performance of vector embeddings, particularly in natural language processing tasks, you may want to explore binary quantized vector embeddings due to their efficiency, high retrieval accuracy, and significant speed improvements for similarity searches.\n"
     ]
    }
   ],
   "source": [
    "print('Question:', question)\n",
    "print('Basic context derived from the vector database:', basic_context)\n",
    "print('Answer:', basic_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, you should consider using TiDB Serverless Vector Storage, as mentioned in the article about pingcap/autoflow. This platform is designed for efficient data management and storage, and it integrates well with conversational AI solutions, making it suitable for applications requiring advanced search capabilities and interaction. Additionally, it can support technologies like Retrieval-Augmented Generation (RAG), enhancing the performance of your vector-based tasks.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT, USER_PROMPT = prompt_template(hybrid_context, question)\n",
    "hybrid_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "print(hybrid_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which vector database should I use?\n",
      "Hybrid context derived from the vector database: \n",
      "<START Article Number: 40>\n",
      "Title: Binary vector embeddings are so cool\n",
      "URL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\n",
      "Summary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \n",
      "\n",
      "- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\n",
      "- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\n",
      "- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\n",
      "- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\n",
      "Description: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 40>\n",
      "\n",
      "\n",
      "<START Article Number: 41>\n",
      "Title: Understanding the BM25 full text search algorithm\n",
      "URL: https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\n",
      "Summary: BM25 (Best Match 25) is a prominent full-text search algorithm utilized in systems like Lucene, Elasticsearch, and SQLite, known for its effectiveness in ranking documents based on their relevance to a query. It operates on the principle of probabilistic ranking, leveraging components such as Inverse Document Frequency (IDF), term frequency, and document length normalization to compute scores for documents. The algorithm's clever design allows it to rank documents without needing to calculate exact probabilities, making it practical for real-world applications. Notably, BM25 scores can only be compared within the same document collection, as they depend on the specific characteristics of that collection. This understanding is crucial for developers looking to implement or enhance search functionalities in applications, particularly when integrating full-text search with vector similarity search for improved content retrieval.\n",
      "\n",
      "### Key Points:\n",
      "- **BM25 Algorithm**: A widely adopted full-text search algorithm that ranks documents based on relevance.\n",
      "- **Components**: Utilizes query terms, IDF, term frequency, and document length normalization.\n",
      "- **Probabilistic Ranking**: Focuses on the order of documents rather than exact relevance probabilities.\n",
      "- **Score Comparisons**: BM25 scores can only be compared within the same document collection due to dependency on collection-specific metrics.\n",
      "- **Hybrid Search**: Increasingly used in conjunction with vector similarity search to enhance search capabilities.\n",
      "Description: BM25 is a widely used algorithm for full text search. I wanted to understand how it works, so here is my attempt at understanding by re-explaining.\n",
      "Created: 2024-11-20\n",
      "Type: web-page\n",
      "<END Article Number: 41>\n",
      "\n",
      "\n",
      "<START Article Number: 21>\n",
      "Title: GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai\n",
      "URL: https://github.com/pingcap/autoflow\n",
      "Summary: pingcap/autoflow is an open-source conversational knowledge base tool leveraging Graph RAG (Retrieval-Augmented Generation) architecture, built on TiDB Serverless Vector Storage. It integrates advanced features such as a perplexity-style conversational search, a built-in website crawler for comprehensive documentation coverage, and an embeddable JavaScript snippet for seamless integration into existing websites. The tech stack includes TiDB for data storage, LlamaIndex for RAG framework, and DSPy for programming foundation models. Key functionalities include the ability to edit the knowledge graph for accuracy, support for multiple knowledge bases, and performance enhancements in CI processes. This tool is particularly relevant for developers looking to implement conversational AI solutions and enhance user interaction through intelligent search capabilities.\n",
      "\n",
      "- Utilize Graph RAG architecture for enhanced conversational AI.\n",
      "- Implement a built-in website crawler for improved documentation search.\n",
      "- Integrate an embeddable JavaScript snippet for user-friendly interfaces.\n",
      "- Leverage TiDB for efficient data management and storage.\n",
      "- Contribute to the project by following community guidelines.\n",
      "Description: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai - pingcap/autoflow\n",
      "Created: 2024-11-22\n",
      "Type: web-page\n",
      "<END Article Number: 21>\n",
      "\n",
      "Answer: Based on the provided context, you should consider using TiDB Serverless Vector Storage, as mentioned in the article about pingcap/autoflow. This platform is designed for efficient data management and storage, and it integrates well with conversational AI solutions, making it suitable for applications requiring advanced search capabilities and interaction. Additionally, it can support technologies like Retrieval-Augmented Generation (RAG), enhancing the performance of your vector-based tasks.\n"
     ]
    }
   ],
   "source": [
    "print('Question:', question)\n",
    "print('Hybrid context derived from the vector database:', hybrid_context)\n",
    "print('Answer:', hybrid_response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
