{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the env_vars module\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from modules.env_vars import set_os_env_vars, check_missing_vars\n",
    "from modules.neon_db import run_neon_query, load_sql_query\n",
    "from modules.date_functions import get_current_date\n",
    "from modules.reference_extraction import create_content_from_df\n",
    "from modules.prompt_templates import one_shot_example, system_message_example\n",
    "\n",
    "set_os_env_vars() # This will execute the code in env_vars.py and put the environment variables in os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.langchain_config import set_langsmith_client, get_langsmith_tracer, get_llm_model, load_model_costs\n",
    "\n",
    "set_langsmith_client()\n",
    "tracer = get_langsmith_tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_milvus.retrievers import MilvusCollectionHybridSearchRetriever\n",
    "from langchain_milvus.utils.sparse import BM25SparseEmbedding\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from pymilvus import (\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    WeightedRanker,\n",
    "    connections,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude-3-sonnet-20240229': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'claude-3-5-sonnet-20241022': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'gpt-4o-mini': {'provider': 'openai'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the costs\n",
    "MODEL_COSTS = load_model_costs()\n",
    "MODEL_COSTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dense and sparse embedding functions\n",
    "texts = [\n",
    "    \"In 'The Whispering Walls' by Ava Moreno, a young journalist named Sophia uncovers a decades-old conspiracy hidden within the crumbling walls of an ancient mansion, where the whispers of the past threaten to destroy her own sanity.\",\n",
    "    \"In 'The Last Refuge' by Ethan Blackwood, a group of survivors must band together to escape a post-apocalyptic wasteland, where the last remnants of humanity cling to life in a desperate bid for survival.\",\n",
    "    \"In 'The Memory Thief' by Lila Rose, a charismatic thief with the ability to steal and manipulate memories is hired by a mysterious client to pull off a daring heist, but soon finds themselves trapped in a web of deceit and betrayal.\",\n",
    "    \"In 'The City of Echoes' by Julian Saint Clair, a brilliant detective must navigate a labyrinthine metropolis where time is currency, and the rich can live forever, but at a terrible cost to the poor.\",\n",
    "    \"In 'The Starlight Serenade' by Ruby Flynn, a shy astronomer discovers a mysterious melody emanating from a distant star, which leads her on a journey to uncover the secrets of the universe and her own heart.\",\n",
    "    \"In 'The Shadow Weaver' by Piper Redding, a young orphan discovers she has the ability to weave powerful illusions, but soon finds herself at the center of a deadly game of cat and mouse between rival factions vying for control of the mystical arts.\",\n",
    "    \"In 'The Lost Expedition' by Caspian Grey, a team of explorers ventures into the heart of the Amazon rainforest in search of a lost city, but soon finds themselves hunted by a ruthless treasure hunter and the treacherous jungle itself.\",\n",
    "    \"In 'The Clockwork Kingdom' by Augusta Wynter, a brilliant inventor discovers a hidden world of clockwork machines and ancient magic, where a rebellion is brewing against the tyrannical ruler of the land.\",\n",
    "    \"In 'The Phantom Pilgrim' by Rowan Welles, a charismatic smuggler is hired by a mysterious organization to transport a valuable artifact across a war-torn continent, but soon finds themselves pursued by deadly assassins and rival factions.\",\n",
    "    \"In 'The Dreamwalker's Journey' by Lyra Snow, a young dreamwalker discovers she has the ability to enter people's dreams, but soon finds herself trapped in a surreal world of nightmares and illusions, where the boundaries between reality and fantasy blur.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>media_type</th>\n",
       "      <th>status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0762d1d-a825-4427-a785-cb52229f4c67</td>\n",
       "      <td>https://aidanmclaughlin.notion.site/reasoners-...</td>\n",
       "      <td>web-page</td>\n",
       "      <td>completed</td>\n",
       "      <td>2024-11-29 07:51:53.011015</td>\n",
       "      <td>Notion – The all-in-one workspace for your not...</td>\n",
       "      <td>A new tool that blends your everyday work apps...</td>\n",
       "      <td>The article discusses the limitations of curre...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  b0762d1d-a825-4427-a785-cb52229f4c67   \n",
       "\n",
       "                                                 url media_type     status  \\\n",
       "0  https://aidanmclaughlin.notion.site/reasoners-...   web-page  completed   \n",
       "\n",
       "                  created_at  \\\n",
       "0 2024-11-29 07:51:53.011015   \n",
       "\n",
       "                                               title  \\\n",
       "0  Notion – The all-in-one workspace for your not...   \n",
       "\n",
       "                                         description  \\\n",
       "0  A new tool that blends your everyday work apps...   \n",
       "\n",
       "                                             summary author published_at  \n",
       "0  The article discusses the limitations of curre...   None          NaT  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = load_sql_query(\"web_pages.sql\")\n",
    "df = run_neon_query(query)\n",
    "\n",
    "print(\"Number of rows:\", len(df.index))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "\n",
      "<START Article Number: 1>\n",
      "Title: Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.\n",
      "URL: https://aidanmclaughlin.notion.site/reasoners-problem\n",
      "Summary: The article discusses the limitations of current reasoning models, particularly OpenAI's o1, which utilize reinforcement learning (RL) to enhance reasoning capabilities. While these models show promise in structured environments with clear rewards, they struggle with open-ended tasks that lack frequent feedback, such as creative writing or philosophical reasoning. The author argues that despite the advancements in RL, these models do not generalize well beyond their training domains, leading to subpar performance in tasks requiring nuanced understanding. The piece highlights the challenges of scaling model size and the potential stagnation in AI development if the focus remains solely on improving reasoning without addressing the need for larger, more capable models. Key insights include the importance of transfer learning, the limitations of RL in sparse reward environments, and the need for models that can handle complex, unstructured tasks effectively.\n",
      "\n",
      "- Recognize that RL-based models excel in environments with clear rewards but falter in open-ended tasks.\n",
      "- Understand the limitations of transfer learning in current reasoning models, which do not generalize well across different domains.\n",
      "- Acknowledge the challenges in scaling model size and the potential for stagnation in AI advancements if focus remains narrow.\n",
      "- Consider the importance of developing models that can effectively tackle complex, unstructured problems beyond mathematical or coding tasks.\n",
      "Description: A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\n",
      "Created: 2024-11-29\n",
      "Type: web-page\n",
      "<END Article Number: 1>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the results (summary, titles, etc.)\n",
    "all_content, all_content_list, all_content_dict = create_content_from_df(df)\n",
    "print(len(all_content_list))\n",
    "print(all_content_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dense and sparse embedding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_embedding_func = OpenAIEmbeddings()\n",
    "dense_dim = len(dense_embedding_func.embed_query(all_content_list[1]))\n",
    "dense_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.7702225,\n",
       " 1: 1.540445,\n",
       " 2: 1.540445,\n",
       " 4: 0.7702225,\n",
       " 10: 0.83173335,\n",
       " 13: 0.7702225,\n",
       " 14: 0.7702225,\n",
       " 16: 0.7702225,\n",
       " 19: 2.5301633,\n",
       " 21: 1.540445,\n",
       " 22: 0.9921288,\n",
       " 24: 0.9105601,\n",
       " 30: 0.7702225,\n",
       " 31: 0.7702225,\n",
       " 49: 0.19735943,\n",
       " 55: 1.69029,\n",
       " 57: 0.13119456,\n",
       " 62: 1.2580401,\n",
       " 63: 3.38058,\n",
       " 68: 6.9320025,\n",
       " 77: 5.0603266,\n",
       " 79: 1.0768723,\n",
       " 80: 0.7702225,\n",
       " 85: 0.75527894,\n",
       " 92: 0.9921288,\n",
       " 101: 0.7702225,\n",
       " 110: 0.7702225,\n",
       " 111: 2.799022,\n",
       " 112: 0.7702225,\n",
       " 113: 0.7702225,\n",
       " 114: 0.7702225,\n",
       " 115: 5.598044,\n",
       " 116: 2.799022,\n",
       " 117: 1.9647787,\n",
       " 118: 3.6393967,\n",
       " 119: 2.5301633,\n",
       " 120: 3.6805112,\n",
       " 121: 2.5301633,\n",
       " 122: 1.9647787,\n",
       " 123: 0.7702225,\n",
       " 124: 1.4596256,\n",
       " 125: 0.7702225,\n",
       " 126: 2.5301633,\n",
       " 127: 6.305472,\n",
       " 128: 6.305472,\n",
       " 129: 3.1411963,\n",
       " 130: 3.8511124,\n",
       " 131: 5.3843617,\n",
       " 132: 7.3610225,\n",
       " 133: 3.38058,\n",
       " 134: 0.394008,\n",
       " 135: 7.3610225,\n",
       " 136: 6.305472,\n",
       " 137: 4.62327,\n",
       " 138: 1.5705981,\n",
       " 139: 2.799022,\n",
       " 140: 6.305472,\n",
       " 141: 3.641494,\n",
       " 142: 7.3610225,\n",
       " 143: 3.152736,\n",
       " 144: 1.5705981,\n",
       " 145: 3.6805112,\n",
       " 146: 2.5301633,\n",
       " 147: 3.6805112,\n",
       " 148: 1.6634667,\n",
       " 149: 6.3791957,\n",
       " 150: 5.598044,\n",
       " 151: 6.305472,\n",
       " 152: 3.6805112,\n",
       " 153: 7.59049,\n",
       " 154: 5.0603266,\n",
       " 155: 6.9349046,\n",
       " 156: 6.305472,\n",
       " 157: 3.9295573,\n",
       " 158: 5.598044,\n",
       " 159: 2.799022,\n",
       " 160: 5.0603266,\n",
       " 161: 7.59049,\n",
       " 162: 3.152736,\n",
       " 163: 7.3610225,\n",
       " 164: 7.3610225,\n",
       " 165: 1.3558352,\n",
       " 166: 3.6805112,\n",
       " 167: 3.152736,\n",
       " 168: 7.3610225,\n",
       " 169: 2.7116704,\n",
       " 170: 4.62327,\n",
       " 171: 1.5705981,\n",
       " 172: 2.311635,\n",
       " 173: 3.6805112,\n",
       " 174: 4.7117944,\n",
       " 175: 3.152736,\n",
       " 176: 3.9295573,\n",
       " 177: 0.60824776,\n",
       " 178: 2.799022,\n",
       " 179: 3.6805112,\n",
       " 180: 1.9647787,\n",
       " 181: 3.6805112,\n",
       " 182: 1.1653037,\n",
       " 183: 2.1263986,\n",
       " 184: 3.6805112,\n",
       " 185: 2.5301633,\n",
       " 186: 2.5301633,\n",
       " 188: 0.0,\n",
       " 189: 3.152736,\n",
       " 190: 2.5301633,\n",
       " 191: 0.60824776,\n",
       " 192: 3.6805112,\n",
       " 193: 1.5705981,\n",
       " 194: 3.152736,\n",
       " 195: 3.152736,\n",
       " 196: 2.5301633,\n",
       " 197: 3.6805112,\n",
       " 198: 3.152736}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_embedding_func = BM25SparseEmbedding(corpus=all_content_list)\n",
    "sparse_embedding_func.embed_query(all_content_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Milvus Collection and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection URI and establish connection\n",
    "connections.connect(uri=\"./milvus_demo.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define field names and their data types\n",
    "pk_field = \"doc_id\"\n",
    "dense_field = \"dense_vector\"\n",
    "sparse_field = \"sparse_vector\"\n",
    "text_field = \"text\"\n",
    "fields = [\n",
    "    FieldSchema(\n",
    "        name=pk_field,\n",
    "        dtype=DataType.VARCHAR,\n",
    "        is_primary=True,\n",
    "        auto_id=True,\n",
    "        max_length=100,\n",
    "    ),\n",
    "    FieldSchema(name=dense_field, dtype=DataType.FLOAT_VECTOR, dim=dense_dim),\n",
    "    FieldSchema(name=sparse_field, dtype=DataType.SPARSE_FLOAT_VECTOR),\n",
    "    FieldSchema(name=text_field, dtype=DataType.VARCHAR, max_length=65_535),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection with the defined schema\n",
    "schema = CollectionSchema(fields=fields, enable_dynamic_field=False)\n",
    "collection = Collection(\n",
    "    name=\"IntroductionToTheNovels\", schema=schema, consistency_level=\"Strong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index for dense and sparse vectors\n",
    "dense_index = {\"index_type\": \"FLAT\", \"metric_type\": \"IP\"}\n",
    "collection.create_index(\"dense_vector\", dense_index)\n",
    "sparse_index = {\"index_type\": \"SPARSE_INVERTED_INDEX\", \"metric_type\": \"IP\"}\n",
    "collection.create_index(\"sparse_vector\", sparse_index)\n",
    "\n",
    "# Flush the collection to make the changes persistent\n",
    "collection.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert entities into the collection and load the collection\n",
    "entities = []\n",
    "for text in all_content_list:\n",
    "    entity = {\n",
    "        dense_field: dense_embedding_func.embed_documents([text])[0],\n",
    "        sparse_field: sparse_embedding_func.embed_documents([text])[0],\n",
    "        text_field: text,\n",
    "    }\n",
    "    entities.append(entity)\n",
    "collection.insert(entities)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can instantiate our retriever, defining search parameters for sparse and dense fields:\n",
    "sparse_search_params = {\"metric_type\": \"IP\"}\n",
    "dense_search_params = {\"metric_type\": \"IP\", \"params\": {}}\n",
    "# In the input parameters of this Retriever, we use a dense embedding and a sparse embedding to perform hybrid search on the two fields of this Collection, and use WeightedRanker for reranking. Finally, 3 top-K Documents will be returned.\n",
    "retriever = MilvusCollectionHybridSearchRetriever(\n",
    "    collection=collection,\n",
    "    rerank=WeightedRanker(0.5, 0.5),\n",
    "    anns_fields=[dense_field, sparse_field],\n",
    "    field_embeddings=[dense_embedding_func, sparse_embedding_func],\n",
    "    field_search_params=[dense_search_params, sparse_search_params],\n",
    "    top_k=3,\n",
    "    text_field=text_field,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '454566879038537787'}, page_content='\\n<START Article Number: 40>\\nTitle: Binary vector embeddings are so cool\\nURL: https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\nSummary: Binary quantized vector embeddings represent a significant advancement in the field of machine learning, particularly in natural language processing. These embeddings can achieve over 95% retrieval accuracy while compressing data by 32 times and accelerating retrieval speed by approximately 25 times. By converting 32-bit floating point weights to single bits, binary quantization retains essential information, allowing for efficient similarity searches using Hamming distance instead of cosine similarity. This technique, when combined with Matryoshka embeddings—which prioritize important information at the beginning of the vector—further enhances performance. The results show that binary embeddings not only reduce storage costs but also improve computational efficiency, making them a compelling choice for applications requiring fast and accurate vector similarity searches. Key insights include the effectiveness of binary quantization in maintaining high accuracy with minimal data size, and the potential for significant speed improvements in distance calculations. \\n\\n- Utilize binary quantized embeddings to achieve high retrieval accuracy with reduced data size.\\n- Implement Hamming distance for faster similarity searches compared to traditional cosine similarity.\\n- Explore the combination of binary quantization with Matryoshka embeddings for enhanced performance.\\n- Consider the computational efficiency of binary embeddings in applications requiring rapid data processing.\\nDescription: Vector embeddings by themselves are pretty neat. Binary quantized vector embeddings are extra impressive. In short, they can retain 95+% retrieval accuracy with 32x compression 🤯.\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 40>\\n'),\n",
       " Document(metadata={'doc_id': '454566879038537785'}, page_content=\"\\n<START Article Number: 38>\\nTitle: GitHub - langflow-ai/langflow: Langflow is a low-code app builder for RAG and multi-agent AI applications. It’s Python-based and agnostic to any model, API, or database.\\nURL: https://github.com/langflow-ai/langflow\\nSummary: Langflow is a low-code application builder designed for Retrieval-Augmented Generation (RAG) and multi-agent AI applications, built on Python and agnostic to any specific model, API, or database. It features a visual IDE for drag-and-drop workflow creation, a playground for real-time testing, and supports multi-agent orchestration. Key functionalities include the ability to publish workflows as APIs, integrate with observability tools, and utilize enterprise-grade security through the DataStax Langflow cloud service. The platform is designed for rapid deployment, allowing users to start with a free cloud service or self-manage their installations. \\n\\n### Key Points for Technical Professionals and Product Developers:\\n- **Low-Code Development**: Streamline application development with a visual interface.\\n- **Multi-Agent Support**: Facilitate complex interactions and orchestration among multiple agents.\\n- **API Publishing**: Easily convert workflows into APIs for broader integration.\\n- **Observability Integration**: Enhance monitoring and debugging with tools like LangSmith and LangFuse.\\n- **Flexible Deployment**: Options for cloud-based or self-managed installations to suit different organizational needs.\\n- **Python-Based**: Leverage Python's capabilities while remaining agnostic to specific technologies.\\nDescription: Langflow is a low-code app builder for RAG and multi-agent AI applications. It’s Python-based and agnostic to any model, API, or database. - langflow-ai/langflow\\nCreated: 2024-11-20\\nType: web-page\\n<END Article Number: 38>\\n\"),\n",
       " Document(metadata={'doc_id': '454566879038537764'}, page_content='\\n<START Article Number: 17>\\nTitle: The AI agents stack  | Letta\\nURL: https://www.letta.com/blog/ai-agents-stack\\nSummary: The AI agents stack has evolved significantly, reflecting advancements in memory, tool usage, and deployment strategies. This stack is categorized into three layers: model serving, storage, and agent frameworks. The transition from LLMs to LLM agents highlights the complexity of state management and tool execution, which are critical for developing autonomous systems. Key players in model serving include OpenAI and Anthropic for closed APIs, while vLLM and Ollama cater to local inference needs. Storage solutions like Chroma and Pinecone support the stateful nature of agents, enabling them to retain conversation histories and external data. The ability to call tools through structured outputs distinguishes agents from traditional chatbots, necessitating secure execution environments. Frameworks like Letta and LangChain manage agent state and context, with varying approaches to memory management and cross-agent communication. The future of agent deployment is anticipated to shift towards service-oriented architectures, emphasizing REST APIs for scalability and state normalization. As the ecosystem matures, the choice of frameworks will become increasingly critical for developers building complex agent applications.\\n\\n- Understand the three layers of the AI agents stack: model serving, storage, and agent frameworks.\\n- Recognize the importance of state management and tool execution in developing LLM agents.\\n- Explore model serving options, including both closed APIs and local inference solutions.\\n- Utilize vector databases for effective storage of agent state and conversation history.\\n- Implement secure execution environments for tool calls made by agents.\\n- Choose frameworks based on their state management, memory handling, and support for open models.\\n- Prepare for a shift towards service-oriented architectures for agent deployment, focusing on REST APIs.\\nDescription: Understanding the AI agents stack landscape.\\nCreated: 2024-11-23\\nType: web-page\\n<END Article Number: 17>\\n')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retriever.invoke(\"What are the story about ventures?\")\n",
    "retriever.invoke(\"Which vector database should I use?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use within a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatOpenAI and define a prompt template\n",
    "# llm = ChatOpenAI()\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "streaming = False\n",
    "llm = get_llm_model(model_name, streaming, MODEL_COSTS)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Human: You are an AI assistant, and provides answers to questions by using fact based and statistical information when possible.\n",
    "Use the following pieces of information to provide a concise answer to the question enclosed in <question> tags.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for formatting documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chain using the retriever and other components\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The choice of vector database depends on your specific needs, but popular options include Chroma and Pinecone. Both support the stateful nature of AI agents, enabling them to retain conversation histories and external data. If you require local inference capabilities, consider using vLLM or Ollama for model serving. Evaluate your requirements for scalability, state management, and integration with your existing systems to make the best decision.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform a query using the defined chain\n",
    "# rag_chain.invoke(\"What novels has Lila written and what are their contents?\")\n",
    "rag_chain.invoke(\"Which vector database should I use?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the collection\n",
    "# collection.drop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
