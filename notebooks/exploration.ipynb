{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing gradio for the UI\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the env_vars module\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from modules.env_vars import set_os_env_vars, check_missing_vars\n",
    "from modules.neon_db import run_neon_query, load_sql_query\n",
    "from modules.date_functions import get_current_date\n",
    "\n",
    "import importlib\n",
    "\n",
    "import modules.reference_extraction\n",
    "importlib.reload(modules.reference_extraction)\n",
    "from modules.reference_extraction import create_content_from_df\n",
    "from modules.prompt_templates import one_shot_example, system_message_example\n",
    "\n",
    "set_os_env_vars() # This will execute the code in env_vars.py and put the environment variables in os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.langchain_config import set_langsmith_client, get_langsmith_tracer, get_llm_model, load_model_costs\n",
    "\n",
    "set_langsmith_client()\n",
    "tracer = get_langsmith_tracer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'claude-3-sonnet-20240229': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'claude-3-5-sonnet-20241022': {'provider': 'anthropic',\n",
       "  'input': 0.003,\n",
       "  'output': 0.015},\n",
       " 'gpt-4o-mini': {'provider': 'openai'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the costs\n",
    "MODEL_COSTS = load_model_costs()\n",
    "MODEL_COSTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the language model\n",
    "model_name = \"claude-3-5-sonnet-20241022\"\n",
    "# model_name = \"gpt-4o-mini\"\n",
    "streaming = True # Streaming is when the LLM returns a token at a time, instead of the entire response at once\n",
    "\n",
    "# Initialize the language model\n",
    "llm = get_llm_model(model_name, streaming, MODEL_COSTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>media_type</th>\n",
       "      <th>status</th>\n",
       "      <th>created_at</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>author</th>\n",
       "      <th>published_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b0762d1d-a825-4427-a785-cb52229f4c67</td>\n",
       "      <td>https://aidanmclaughlin.notion.site/reasoners-...</td>\n",
       "      <td>web-page</td>\n",
       "      <td>completed</td>\n",
       "      <td>2024-11-29 07:51:53.011015</td>\n",
       "      <td>Notion ‚Äì The all-in-one workspace for your not...</td>\n",
       "      <td>A new tool that blends your everyday work apps...</td>\n",
       "      <td>The article discusses the limitations of curre...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  b0762d1d-a825-4427-a785-cb52229f4c67   \n",
       "\n",
       "                                                 url media_type     status  \\\n",
       "0  https://aidanmclaughlin.notion.site/reasoners-...   web-page  completed   \n",
       "\n",
       "                  created_at  \\\n",
       "0 2024-11-29 07:51:53.011015   \n",
       "\n",
       "                                               title  \\\n",
       "0  Notion ‚Äì The all-in-one workspace for your not...   \n",
       "\n",
       "                                         description  \\\n",
       "0  A new tool that blends your everyday work apps...   \n",
       "\n",
       "                                             summary author published_at  \n",
       "0  The article discusses the limitations of curre...   None          NaT  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = load_sql_query(\"web_pages.sql\")\n",
    "df = run_neon_query(query)\n",
    "\n",
    "print(\"Number of rows:\", len(df.index))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "\n",
      "<START Article Number: 1>\n",
      "Title: Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases.\n",
      "URL: https://aidanmclaughlin.notion.site/reasoners-problem\n",
      "Summary: The article discusses the limitations of current reasoning models, particularly OpenAI's o1, which utilize reinforcement learning (RL) to enhance reasoning capabilities. While these models show promise in structured environments with clear rewards, they struggle with open-ended tasks that lack frequent feedback, such as creative writing or philosophical reasoning. The author argues that despite the advancements in RL, these models do not generalize well beyond their training domains, leading to subpar performance in tasks requiring nuanced understanding. The piece highlights the challenges of scaling model size and the potential stagnation in AI development if the focus remains solely on improving reasoning without addressing the need for larger, more capable models. Key insights include the importance of transfer learning, the limitations of RL in sparse reward environments, and the need for models that can handle complex, unstructured tasks effectively.\n",
      "\n",
      "- Recognize that RL-based models excel in environments with clear rewards but falter in open-ended tasks.\n",
      "- Understand the limitations of transfer learning in current reasoning models, which do not generalize well across different domains.\n",
      "- Acknowledge the challenges in scaling model size and the potential for stagnation in AI advancements if focus remains narrow.\n",
      "- Consider the importance of developing models that can effectively tackle complex, unstructured problems beyond mathematical or coding tasks.\n",
      "Description: A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\n",
      "Created: 2024-11-29\n",
      "Type: web-page\n",
      "<END Article Number: 1>\n",
      "\n",
      "dict_keys(['title', 'url', 'summary', 'description', 'created_at', 'media_type', 'content'])\n",
      "{'title': 'Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases.', 'url': 'https://aidanmclaughlin.notion.site/reasoners-problem', 'summary': \"The article discusses the limitations of current reasoning models, particularly OpenAI's o1, which utilize reinforcement learning (RL) to enhance reasoning capabilities. While these models show promise in structured environments with clear rewards, they struggle with open-ended tasks that lack frequent feedback, such as creative writing or philosophical reasoning. The author argues that despite the advancements in RL, these models do not generalize well beyond their training domains, leading to subpar performance in tasks requiring nuanced understanding. The piece highlights the challenges of scaling model size and the potential stagnation in AI development if the focus remains solely on improving reasoning without addressing the need for larger, more capable models. Key insights include the importance of transfer learning, the limitations of RL in sparse reward environments, and the need for models that can handle complex, unstructured tasks effectively.\\n\\n- Recognize that RL-based models excel in environments with clear rewards but falter in open-ended tasks.\\n- Understand the limitations of transfer learning in current reasoning models, which do not generalize well across different domains.\\n- Acknowledge the challenges in scaling model size and the potential for stagnation in AI advancements if focus remains narrow.\\n- Consider the importance of developing models that can effectively tackle complex, unstructured problems beyond mathematical or coding tasks.\", 'description': \"A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\", 'created_at': '2024-11-29', 'media_type': 'web-page', 'content': \"\\n<START Article Number: 1>\\nTitle: Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases.\\nURL: https://aidanmclaughlin.notion.site/reasoners-problem\\nSummary: The article discusses the limitations of current reasoning models, particularly OpenAI's o1, which utilize reinforcement learning (RL) to enhance reasoning capabilities. While these models show promise in structured environments with clear rewards, they struggle with open-ended tasks that lack frequent feedback, such as creative writing or philosophical reasoning. The author argues that despite the advancements in RL, these models do not generalize well beyond their training domains, leading to subpar performance in tasks requiring nuanced understanding. The piece highlights the challenges of scaling model size and the potential stagnation in AI development if the focus remains solely on improving reasoning without addressing the need for larger, more capable models. Key insights include the importance of transfer learning, the limitations of RL in sparse reward environments, and the need for models that can handle complex, unstructured tasks effectively.\\n\\n- Recognize that RL-based models excel in environments with clear rewards but falter in open-ended tasks.\\n- Understand the limitations of transfer learning in current reasoning models, which do not generalize well across different domains.\\n- Acknowledge the challenges in scaling model size and the potential for stagnation in AI advancements if focus remains narrow.\\n- Consider the importance of developing models that can effectively tackle complex, unstructured problems beyond mathematical or coding tasks.\\nDescription: A new tool that blends your everyday work apps into one. It's the all-in-one workspace for you and your team\\nCreated: 2024-11-29\\nType: web-page\\n<END Article Number: 1>\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Print out the results (summary, titles, etc.)\n",
    "all_content, all_content_list, all_content_dict = create_content_from_df(df)\n",
    "\n",
    "print(len(all_content_list))\n",
    "print(all_content_list[0])\n",
    "print(all_content_dict[1].keys())\n",
    "print(all_content_dict[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[1] Notion ‚Äì The all-in-one workspace for your notes, tasks, wikis, and databases. | https://aidanmclaughlin.notion.site/reasoners-problem\\n[2] LangChain State of AI Agents Report | https://www.langchain.com/stateofaiagents\\n[3] How to Get Started in AI Consulting: - Jason Liu | https://jxnl.co/writing/2024/10/31/consulting-start/\\n[4] GitHub - abus-aikorea/voice-pro: Comprehensive Gradio WebUI for audio processing, powered by Whisper engines (Whisper, Faster-Whisper, Whisper-Timestamped). Features Voice Changer, zero-shot Voice Cloning (E2, F5-TTS), YouTube downloading, vocal isolation(UVR5), Text-to-Speech (Edge-TTS), and multi-language translation. Perfect for content creators and developers. | https://github.com/abus-aikorea/voice-pro\\n[5] Creating a LLM-as-a-Judge That Drives Business Results ‚Äì | https://hamel.dev/blog/posts/llm-judge/\\n[6] GitHub - elicit/machine-learning-list: A curriculum for learning about foundation models, from scratch to the frontier | https://github.com/elicit/machine-learning-list\\n[7] Introducing the Model Context Protocol | https://www.anthropic.com/news/model-context-protocol\\n[8] Say What You Mean: A Response to 'Let Me Speak Freely' | https://blog.dottxt.co/say-what-you-mean.html\\n[9] GitHub - OpenInterpreter/open-interpreter: A natural language interface for computers | https://github.com/OpenInterpreter/open-interpreter\\n[10] GitHub - huggingface/smollm: Everything about the SmolLM & SmolLM2 family of models | https://github.com/huggingface/smollm\\n[11] Archetypes of LLM apps | https://www.contraption.co/archetypes-of-llm-apps/\\n[12] Inside Large Language Models: How AI Really Understands Language | https://open.substack.com/pub/diamantai/p/inside-large-language-models-how\\n[13] Put AI to Work for You With Runner H | https://www.hcompany.ai/blog/introducing-h\\n[14] Put AI to Work for You - Runner H | https://www.hcompany.ai/\\n[15] A computer for your AI | Scrapybara | https://scrapybara.com/\\n[16] Introducing Agent Blocks: Build AI Workflows That Scale Through Multi-Agent Collaboration | https://agpt.co/blog/introducing-agent-blocks\\n[17] The AI agents stack  | Letta | https://www.letta.com/blog/ai-agents-stack\\n[18] GitHub - addyosmani/git2txt: CLI tool to convert GitHub repositories to text files for LLMs | https://github.com/addyosmani/git2txt\\n[19] How to Use a Proxy with Node-Fetch in 2024 - ZenRows | https://www.zenrows.com/blog/node-fetch-proxy#prerequisite\\n[20] Bayesian Neural Networks | https://www.cs.toronto.edu/~duvenaud/distill_bayes_net/public/\\n[21] GitHub - pingcap/autoflow: pingcap/autoflow is a Graph RAG based and conversational knowledge base tool built with TiDB Serverless Vector Storage. Demo: https://tidb.ai | https://github.com/pingcap/autoflow\\n[22]  | https://arxiv.org/pdf/2404.12272\\n[23] How to use NotebookLM for personalized knowledge synthesis | https://open.substack.com/pub/aisupremacy/p/how-to-use-notebooklm-for-personalized\\n[24] OpenAI Realtime API: The Missing Manual | https://www.latent.space/p/realtime-api\\n[25] AI Giants Rethink Model Training Strategy as Scaling Laws Break Down | https://www.deeplearning.ai/the-batch/ai-giants-rethink-model-training-strategy-as-scaling-laws-break-down/\\n[26]  | https://arxiv.org/pdf/2411.10109\\n[27] Collaborative Intelligence | Union Square Ventures | https://www.usv.com/writing/2024/11/collaborative-intelligence/\\n[28] Reasoning with Language Model is Planning with World Model | https://arxiv.org/abs/2305.14992\\n[29] GitHub - Arize-ai/phoenix: AI Observability & Evaluation | https://github.com/Arize-ai/phoenix\\n[30] Modal: Serverless cloud infrastructure for AI, ML, and data | https://modal.com/\\n[31] EMNLP 2024 Tutorial: Language Agents: Foundations, Prospects, and Risks | https://language-agent-tutorial.github.io/\\n[32] GitHub - AkariAsai/OpenScholar: This repository includes the official implementation of OpenScholar: Synthesizing Scientific Literature with Retrieval-augmented LMs. | https://github.com/AkariAsai/OpenScholar\\n[33] GitHub - ai16z/eliza: Conversational Agent for Twitter and Discord | https://github.com/ai16z/eliza\\n[34] GitHub - anselale/Dignity | https://github.com/anselale/Dignity\\n[35] GitHub - DataBassGit/o7: Agent framework for generating a synthetic dataset. This will be raw CoT and Reflection output to be cleaned up by a later step. | https://github.com/DataBassGit/o7\\n[36] GitHub - DataBassGit/AgentForge: Extensible AGI Framework | https://github.com/DataBassGit/AgentForge\\n[37] GitHub - DocumindHQ/documind: Open-source platform for extracting structured data from documents using AI. | https://github.com/DocumindHQ/documind\\n[38] GitHub - langflow-ai/langflow: Langflow is a low-code app builder for RAG and multi-agent AI applications. It‚Äôs Python-based and agnostic to any model, API, or database. | https://github.com/langflow-ai/langflow\\n[39] Comparing full text search algorithms: BM25, TF-IDF, and Postgres | https://emschwartz.me/comparing-full-text-search-algorithms-bm25-tf-idf-and-postgres/\\n[40] Binary vector embeddings are so cool | https://emschwartz.me/binary-vector-embeddings-are-so-cool/\\n[41] Understanding the BM25 full text search algorithm | https://emschwartz.me/understanding-the-bm25-full-text-search-algorithm/\\n[42] Agent Protocol: Interoperability for LLM agents | https://blog.langchain.dev/agent-protocol-interoperability-for-llm-agents/\\n[43] The Most Dangerous Thing An AI Startup Can Do Is Build For Other AI Startups | https://www.latent.space/p/enterprise?utm_campaign=post&utm_medium=web\\n[44] A statistical approach to model evaluations | https://www.anthropic.com/research/statistical-approach-to-model-evals\\n[45] GitHub - DataExpert-io/data-engineer-handbook: This is a repo with links to everything you'd ever want to learn about data engineering | https://github.com/DataExpert-io/data-engineer-handbook\\n[46] How Meta Uses LLMs to Improve Incident Response (and how you can too) - Parity | https://www.tryparity.com/blog/how-meta-uses-llms-to-improve-incident-response\\n[47] GitHub - vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs | https://github.com/vllm-project/vllm\\n[48] Multi-agent Systems | https://langchain-ai.github.io/langgraphjs/concepts/multi_agent/#supervisor\\n[49] Agent architectures | https://langchain-ai.github.io/langgraphjs/concepts/agentic_concepts/\\n[50] TigerEye - Sales and Go-to-Market Planning | https://www.tigereye.com/\\n[51] Full-Stack Web Scraping API & Data Extraction Services  | Zyte | https://www.zyte.com/\\n[52] GitHub - PKU-YuanGroup/LLaVA-o1 | https://github.com/PKU-YuanGroup/LLaVA-o1\\n[53] GitHub - circlemind-ai/fast-graphrag: RAG that intelligently adapts to your use case, data, and queries | https://github.com/circlemind-ai/fast-graphrag\\n[54] SPC's Request for Curiosity | https://blog.southparkcommons.com/request-for-curiosity/\\n[55] GitHub - exo-explore/exo: Run your own AI cluster at home with everyday devices üì±üíª üñ•Ô∏è‚åö | https://github.com/exo-explore/exo\\n[56] GitHub - khoj-ai/khoj: Your AI second brain. Self-hostable. Get answers from the web or your docs. Build custom agents, schedule automations, do deep research. Turn any online or local LLM into your personal, autonomous AI (e.g gpt, claude, gemini, llama, qwen, mistral). | https://github.com/khoj-ai/khoj\\n[57] Agents @ Work: Lindy.ai | https://www.latent.space/p/lindy?utm_campaign=post&utm_medium=web\\n[58] voyage-multimodal-3: all-in-one embedding model for interleaved text, images, and screenshots | https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/\\n[59] Promptim: an experimental library for prompt optimization | https://blog.langchain.dev/promptim/\\n[60] GitHub - mendableai/firecrawl: üî• Turn entire websites into LLM-ready markdown or structured data. Scrape, crawl and extract with a single API. | https://github.com/mendableai/firecrawl\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare footnotes for the prompt\n",
    "footnotes_text = \"\\n\".join([f\"[{i}] {source['title']} | {source['url']}\" for i, source in all_content_dict.items()])\n",
    "footnotes_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsletter_prompt = one_shot_example(get_current_date())\n",
    "\n",
    "# Create the chain with tracing\n",
    "chain = (newsletter_prompt | llm).with_config(\n",
    "    {\n",
    "        \"callbacks\": [tracer],\n",
    "        \"tags\": [\"newsletter_generation\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "newsletter = chain.invoke({\"context\": all_content, \"current_date\": get_current_date()})\n",
    "print(newsletter.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsletter_prompt = system_message_example()\n",
    "\n",
    "# Create the chain with tracing\n",
    "chain = (newsletter_prompt | llm).with_config(\n",
    "    {\n",
    "        \"callbacks\": [tracer],\n",
    "        \"tags\": [\"newsletter_generation\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test the chain\n",
    "newsletter = chain.invoke({\"context\": all_content, \"current_date\": get_current_date()})\n",
    "print(newsletter.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
